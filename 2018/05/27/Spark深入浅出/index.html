<!DOCTYPE html>
<html>
  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=yes">
  
  
  <title>  Spark深⼊浅出 |   CodingCow </title>

 
  
    <link rel="icon" href="/images/favicon.png">
  


  <link rel="stylesheet" href="/nayo.min.css"> 
</head>  
  <body>   
    
      <header class="header">
	
  <nav class="header-nav">        
   
    <span class="iconfont icon-menu mobile-toggle"></span>   	

    <!-- <a class="header-logo" href="/"></a> -->

    <div class="header-menu">          
              
            

              <a class="header-menu-link" id="header-menu-home" href="/">首页</a>     

            
            
            

              <a class="header-menu-link" id="header-menu-archives" href="/archives">归档</a>     

            
            
            

              <a class="header-menu-link" id="header-menu-categories" href="/categories">分类</a>     

            
            
            

              <a class="header-menu-link" id="header-menu-tags" href="/tags">标签</a>     

            
            
            

              <a class="header-menu-link" id="header-menu-about" href="/about">关于</a>     

            
            
            

              <a class="iconfont icon-menu-search header-menu-link" id="header-menu-search"></a>

            
                
    </div>  
    
  </nav>
</header>   

      <div class="container">       
          
          
            <section class="main">  
          

          <article class="post">
  
	<div class="post-header">

	<p class="post-title">	
		Spark深⼊浅出
	</p>
			

	<div class="meta-info">	
	<span>
		5月 27, 2018
	</span>

	
	
		<i class="iconfont icon-words"></i>
		<span>
			12253
		</span>
	
</div>

</div> 
	 

	  <div class="post-content slideDownMin">

		

			
					<h1 id="～-gt-lt-～"><a href="#～-gt-lt-～" class="headerlink" title="～&gt;_&lt;～"></a>～&gt;_&lt;～</h1><h2 id="Spark-简介"><a href="#Spark-简介" class="headerlink" title="Spark 简介"></a>Spark 简介</h2><p>Apache Spark 是专为⼤规模数据处理⽽设计的快速通⽤的计算引擎。Spark是UC Berkeley AMP lab (加州⼤学伯克利分校AMP实验室)开源的类Hadoop MapReduce的通⽤并⾏框架，Spark拥有Hadoop MapReduce所具有并⾏计算的优点；但不同于MapReduce的是-Job中间输出结果可以保存在内存中，从⽽不再需要读写HDFS，因此Spark能更好地适⽤于数据挖掘与机器学习等需要迭代的MapReduce的算法。<br><img src="/images/spark/spark1.png" alt="img"><br><a id="more"></a></p>
<h4 id="Spark-发展历史"><a href="#Spark-发展历史" class="headerlink" title="Spark  发展历史"></a>Spark  发展历史</h4><p>对于⼀个具有相当技术⻔槛与复杂度的平台，Spark从诞⽣到正式版本的成熟，经历的时间如此之短，让⼈感到惊诧。2009年，Spark诞⽣于伯克利⼤学AMPLab，最开初属于伯克利⼤学的研究性项⽬。它于2010年正式开源，并于2013年成为了Aparch基⾦项⽬，并于2014年成为Aparch基⾦的顶级项⽬，整个过程不到五年时间。Apache Spark是专为⼤规模数据处理⽽设计的快速通⽤的计算引擎 ，现在形成⼀个⾼速发展应⽤⼴泛的⽣态系统。<br><img src="/images/spark/spark2.png" alt="img"><br>Spark提供的基于RDD的⼀体化解决⽅案，将MapReduce、Streaming、SQL、MachineLearning、Graph Processing等模型统⼀到⼀个平台下，并以⼀致的API公开，并提供相同的部署⽅案，使得Spark的⼯程应⽤领域变得更加⼴泛。Spark的特⾊在于它⾸先为⼤数据应⽤提供了⼀个统⼀的平台。从数据处理层⾯看，模型可以分为批处理、交互式、流处理等多种⽅式；⽽从⼤数据平台⽽⾔，已有成熟的Hadoop、Cassandra、Mesos以及其他云的供应商。</p>
<h4 id="Spark-amp-hadoop-关系"><a href="#Spark-amp-hadoop-关系" class="headerlink" title="Spark  &amp;  hadoop 关系"></a>Spark  &amp;  hadoop 关系</h4><p>Spark是对Map Reduce计算模型的改进，也可以理解为没有HDFS和MapReduce，就没有现在的Spark。 Spark可以使⽤YARN作为它的集群管理器，并且可以处理HDFS的数据。这对于已经部署Hadoop集群的⽤户特别重要，毕竟不需要做任何的数据迁移就可以使⽤Spark的强⼤处理能⼒。<br><img src="/images/spark/spark3.png" alt="img"></p>
<h4 id="Spark-计算-amp-Hadoop-MapReduce"><a href="#Spark-计算-amp-Hadoop-MapReduce" class="headerlink" title="Spark 计算&amp;Hadoop MapReduce"></a>Spark 计算&amp;Hadoop MapReduce</h4><p>Hadoop中Map Redcue由Map和Reduce两个阶段，并通过shuffle将两个阶段连接起来的。<br><img src="/images/spark/spark4.png" alt="img"><br>但是套⽤Map Reduce模型解决问题，不得不将问题分解为若⼲个有依赖关系的⼦问题，每个⼦问题对应⼀个Map Reduce作业，最终所有这些作业形成⼀个DAG。Spark是通⽤的DAG框架，可以将多个有依赖关系的作业转换为⼀个⼤的DAG。<br><img src="/images/spark/spark5.png" alt="img"><br>Spark核⼼思想是将Map和Reduce两个操作进⼀步拆分为多个元操作，这些元操作可以灵活组合，产⽣新的操作，并经过⼀些控制程序组装后形成⼀个⼤的DAG作业。由于Hadoop有多个MapReduce作业组成，每个作业都会从HDFS上读取⼀次数据和写⼀次数据（默认写三份），即使这些MapReduce作业产⽣的数据是中间数据也需要写HDFS。这种表达作业依赖关系的⽅式⽐较低效，会浪费⼤量不必要的磁盘和⽹络IO，根本原因是作业之间产⽣的数据不是直接流动的，⽽是借助HDFS作为共享数据存储系统。但是在Spark中，使⽤内存（内存不够使⽤本地磁盘）替代了使⽤HDFS存储中间结果。对于迭代运算效率更⾼。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark6.png" class="lazyload"></p>
<blockquote>
<p>注意：<font color="0099ff" face="黑体">中间结果</font>是指reduce操作后⽣成的结果，所以在⽐较Spark和Hadoop的计算模型的时候⼀般对⽐的是多个阶段的计算。</p>
</blockquote>
<h2 id="Spark-架构介绍"><a href="#Spark-架构介绍" class="headerlink" title="Spark 架构介绍"></a>Spark 架构介绍</h2><p>Spark的整体流程为：Client提交应⽤，Master找到⼀个Worker启动Driver，Driver向Master或者资源管理器申请资源，之后将应⽤转化为RDD Graph，再由DAGScheduler将RDD Graph转化为Stage的有向⽆环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执⾏。在任务执⾏过程中，其他组件协同⼯作，确保整个应⽤顺利执⾏。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark7.png" class="lazyload"><br>Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进⾏控制。在⼀个Spark应⽤的执⾏过程中，Driver和Worker是两个重要⻆⾊。Driver程序是应⽤逻辑执⾏的起点，负责作业的调度，即Task任务的分发，⽽多个Worker⽤来管理计算节点和创建Executor并⾏处理任务。在执⾏阶段，Driver会将Task和Task所依赖的file和jar包序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进⾏处理。</p>
<h4 id="基本组件介绍"><a href="#基本组件介绍" class="headerlink" title="基本组件介绍"></a>基本组件介绍</h4><p><b>ClusterManager:</b> 在Standalone模式中即为Master(主节点)，控制整个集群，监控Worker。在YARN模式中为资源管理器。<br><b>Worker:</b> 从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制。<br><b>Driver:</b> 运⾏Application的main()函数并创建SparkContext。<br><b>RDD:</b> Spark的基本运算单元，⼀组RDD可形成执⾏的有向⽆环图RDD Graph。<br><b>DAGScheduler:</b> 实现将Spark作业分解成⼀到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后⽣成相应的Task Set放到TaskScheduler中。<br><b>TaskScheduler:</b> 将任务(Task)分发给Executor执⾏。<br><b>Stag:</b>  ⼀个Spark作业⼀般包含⼀到多个Stage。<br><b>Task:</b> ⼀个Stage包含⼀到多个Task，通过多个Task实现并⾏运⾏的功能。</p>
<h2 id="Spark-安装部署"><a href="#Spark-安装部署" class="headerlink" title="Spark 安装部署"></a>Spark 安装部署</h2><p><em><strong>1. 安装 CentOS-6.5 64bit 环境，关闭防⽕墙</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS ~]# service iptables stop</span><br><span class="line">iptables: Setting chains to policy ACCEPT: filter [ OK ]</span><br><span class="line">iptables: Flushing firewall rules: [ OK ]</span><br><span class="line">iptables: Unloading modules: [ OK ]</span><br><span class="line">[root@CentOS ~]# chkconfig iptables off</span><br></pre></td></tr></table></figure></p>
<p><i><strong>2. 安装配置 jdk-8u171-linux-x64.rpm 配置 JAVA_HOME 环境变量（/root/.bashrc）</strong></i><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/java/latest</span><br><span class="line">CLASSPATH=.</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export JAVA_HOME</span><br><span class="line">export CLASSPATH</span><br><span class="line">export PATH</span><br><span class="line">[root@CentOS ~]# source .bashrc</span><br></pre></td></tr></table></figure></p>
<p><em><strong>3. 配置 IP 和主机名映射关系</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4</span><br><span class="line">localhost4.localdomain4</span><br><span class="line">::1 localhost localhost.localdomain localhost6</span><br><span class="line">localhost6.localdomain6</span><br><span class="line">192.168.80.128 CentOS</span><br></pre></td></tr></table></figure></p>
<p><em><strong>4. 下载 Spark 安装包<code>spark-2.3.0-bin-hadoop2.6.tgz</code>解压</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS ~]# tar -zxf spark-2.3.0-bin-hadoop2.6.tgz -C /usr</span><br></pre></td></tr></table></figure></p>
<h2 id="Spark-测试"><a href="#Spark-测试" class="headerlink" title="Spark 测试"></a>Spark 测试</h2><p><em><strong>1. 本地测试 Spark</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-shell --master</span><br><span class="line">local[5]</span><br></pre></td></tr></table></figure></p>
<p><em><strong>2. 启动 Spark 集群</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./sbin/start-master.sh 【启动</span><br><span class="line">Master】</span><br><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./sbin/start-slave.sh --cores 2 --memory 512m spark://CentOS:7077 【启动work节点】</span><br><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-shell --master</span><br><span class="line">spark://CentOS:7077 --executor-memory 512M --total-executor-cores 1</span><br></pre></td></tr></table></figure></p>
<h2 id="RDD算⼦"><a href="#RDD算⼦" class="headerlink" title="RDD算⼦"></a>RDD算⼦</h2><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。RDD是Spark的最基本抽象,是对分布式内存的抽象使⽤，实现了以操作本地集合的⽅式来操作分布式数据集的抽象实现。<br>RDD是Spark最核⼼的东⻄，它表示已被分区、不可变的并能够被并⾏操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下⼀个操作可以直接从内存中输⼊，省去了MapReduce⼤量的磁盘IO操作。这对于迭代运算⽐较常⻅的机器学习算法, 交互式数据挖掘来说，效率提升⽐较⼤。</p>
<h4 id="RDD-特点"><a href="#RDD-特点" class="headerlink" title="RDD 特点"></a>RDD 特点</h4><h3 id="RDD计算流程"><a href="#RDD计算流程" class="headerlink" title="RDD计算流程"></a>RDD计算流程</h3><p>Spark的输⼊、运⾏转换、输出。在运⾏转换中通过算⼦对RDD进⾏转换。算⼦是RDD中定义的函数，可以对RDD中的数据进⾏转换和操作。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark8.png" class="lazyload"><br>1、输入：在Spark程序运行中，数据从外部数据空间（例如，HDFS、Scala集合或数据）输入到Spark，数据就进入了Spark运行时数据空间，会转化为Spark中的数据块，通过BlockManager进行管理。<br>2、运行：在Spark数据输入形成RDD后，便可以通过变换算子fliter等，对数据操作并将RDD转化为新的RDD，通过行动（Action）算子，触发Spark提交作业。如果数据需要复用，可以通过Cache算子，将数据缓存到内存。<br>3、输出：程序运行结束数据会输出Spark运行时空间，存储到分布式存储中（如saveAsTextFile输出到HDFS）或Scala数据或集合中（collect输出到Scala集合，count返回Scala Int型数据）。</p>
<h4 id="RDD编程模型"><a href="#RDD编程模型" class="headerlink" title="RDD编程模型"></a>RDD编程模型</h4><p>来看一段代码：textFile算子从HDFS读取日志文件，返回“file”（RDD）；filter算子筛出带 “ERROR” 的行，赋给 “errors”（新RDD）；cache算子把它缓存下来以备未来使用；count算子返回 “errors” 的行数。RDD看起来与Scala集合类型 没有太大差别，但它们的数据和运行模型大相迥异。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> file= sc.textFile(<span class="string">"hdfs://CentOS:9000/demo/src/access.log"</span>)</span><br><span class="line"><span class="keyword">var</span> errors = file.filter(_.contains(<span class="string">"ERROR"</span>))</span><br><span class="line">errors.cache()</span><br><span class="line">errors.count()</span><br></pre></td></tr></table></figure></p>
<p>上⾯代码给出了RDD数据模型，并将上例中⽤到的四个算⼦映射到四种算⼦类型。Spark程序⼯作在两个空间中：Spark RDD空间和Scala原⽣数据空间。在原⽣数据空间⾥，数据表现为标量（scalar，即Scala基本类型，⽤橘⾊⼩⽅块表示）、集合类型（蓝⾊虚线 框）和持久存储（红⾊圆柱）。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark9.png" class="lazyload"></p>
<h3 id="RDD运算逻辑"><a href="#RDD运算逻辑" class="headerlink" title="RDD运算逻辑"></a>RDD运算逻辑</h3><p>在Spark应用中，整个执行流程在逻辑上运算之间会形成有向无环图。Action算子触发之后会将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。Spark的调度方式与MapReduce有所不同。Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数进行流水线执行。图中的A、B、C、D、E、F、G，分别代表不同的RDD，RDD内的一个方框代表一个数据块（分区）。数据从HDFS输入Spark，形成RDD A和RDD C、RDD E，RDD C上执行map操作，转换为RDD D，RDD E和 RDD D 合并形成 RDD F，RDD B和RDD F进行join操作转换为G，而在B到G的过程中又会进行Shuffle。最后RDD G通过函数saveAsSequenceFile输出保存到HDFS中。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark10.png" class="lazyload"></p>
<h4 id="RDD依赖关系（narrow-wide）"><a href="#RDD依赖关系（narrow-wide）" class="headerlink" title="RDD依赖关系（narrow | wide）"></a>RDD依赖关系（narrow | wide）</h4><p>RDD间依赖分为 窄依赖 (narrow dependencies) 和宽依赖 (wide dependencies) 。窄依赖是指父 RDD 的每个分区都只被子RDD的一个分区所使用，例如map、filter。相应的，那么宽依赖就是指父 RDD 的分区被多个子 RDD 的分区所依赖，例如groupByKey、reduceByKey等操作。如果父RDD的一个Partition被一个子RDD的Partition所使用就是窄依赖，否则的话就是宽依赖。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark11.png" class="lazyload"><br>窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在 filter 之后执行map 。其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父 RDD 的分区需要重新计算。而对于宽依赖，一个结点的故障可能导致来自所有父 RDD 的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark 会在持有各个父分区的结点上，将中间数据持久化来简化故障还原，就像 MapReduce 会持久化 map 的输出一样。Spark状态切换是遇到宽依赖就会生成一个State。</p>
<h3 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h3><p>1）创建RDD<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">scala&gt; sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">45</span>),<span class="number">10</span>)</span><br><span class="line">scala&gt; sc.textFile(<span class="string">"/root/hello.txt"</span>)</span><br></pre></td></tr></table></figure></p>
<p>2）RDD算⼦（转换算⼦、Action算⼦）<br>转换算⼦<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark12.png" class="lazyload"><br>Action算⼦<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark13.png" class="lazyload"></p>
<p><strong>map、flatMap、filter⽤法</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2 =sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>)).map(_*<span class="number">2</span>).sortBy(x=&gt;x,<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.filter(_&gt;<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> rdd4 sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>)).map(_*<span class="number">2</span>).sortBy(x=&gt;x+<span class="string">""</span>,<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> rdd5 =</span><br><span class="line">sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">10</span>)).map(_*<span class="number">2</span>).sortBy(x=&gt;x.toString,<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> rdd6 = sc.parallelize(<span class="type">Array</span>(<span class="string">"a b c"</span>, <span class="string">"d e f"</span>, <span class="string">"h i j"</span>))</span><br><span class="line">rdd6.flatMap(_.split(' ')).collect</span><br></pre></td></tr></table></figure></p>
<p><strong>union求并集（类型要⼀致）</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd6 = sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>))</span><br><span class="line"><span class="keyword">val</span> rdd7 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> rdd8 = rdd6.union(rdd7)</span><br><span class="line">rdd8.distinct.sortBy(x=&gt;x).collect</span><br></pre></td></tr></table></figure></p>
<p><strong>intersection求交集</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">1</span>), (<span class="string">"ls"</span>, <span class="number">2</span>), (<span class="string">"ww"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">9</span>), (<span class="string">"win7"</span>, <span class="number">8</span>), (<span class="string">"zl"</span>, <span class="number">7</span>)))</span><br><span class="line">rdd1.intersection(rdd2).collect</span><br></pre></td></tr></table></figure></p>
<p><strong>join操作</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">1</span>), (<span class="string">"ls"</span>, <span class="number">2</span>), (<span class="string">"ww"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">9</span>), (<span class="string">"win7"</span>, <span class="number">8</span>), (<span class="string">"zl"</span>, <span class="number">7</span>)))</span><br><span class="line">rdd1.join(rdd2).collect</span><br><span class="line">rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">rdd1.rightOuterJoin(rdd2).collect</span><br></pre></td></tr></table></figure></p>
<p><strong>groupByKey</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">1</span>), (<span class="string">"ls"</span>, <span class="number">2</span>), (<span class="string">"ww"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">9</span>), (<span class="string">"win7"</span>, <span class="number">8</span>), (<span class="string">"zl"</span>, <span class="number">7</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1 union rdd2</span><br><span class="line">rdd3.groupByKey().map(x =&gt; (x._1,x._2.sum))</span><br></pre></td></tr></table></figure></p>
<p><strong>WordCount</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"/root/words.txt"</span>).flatMap(x=&gt;x.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).sortBy(_._2,<span class="literal">false</span>).collect</span><br><span class="line">sc.textFile(<span class="string">"/root/words.txt"</span>).flatMap(x=&gt;x.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).groupByKey.map(t=&gt;(t._1, t._2.sum)).collect</span><br></pre></td></tr></table></figure></p>
<p><strong>cogroup（相同key聚集在⼀起）</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"tom"</span>, <span class="number">1</span>), (<span class="string">"tom"</span>, <span class="number">2</span>), (<span class="string">"jerry"</span>, <span class="number">3</span>),(<span class="string">"kitty"</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"jerry"</span>, <span class="number">2</span>), (<span class="string">"tom"</span>, <span class="number">1</span>), (<span class="string">"shuke"</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.cogroup(rdd2)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd3.map(t=&gt;(t._1, t._2._1.sum + t._2._2.sum))</span><br></pre></td></tr></table></figure></p>
<p><strong>笛卡尔积</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>(<span class="string">"tom"</span>, <span class="string">"jerry"</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>(<span class="string">"tom"</span>, <span class="string">"kitty"</span>, <span class="string">"shuke"</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.cartesian(rdd2)</span><br></pre></td></tr></table></figure></p>
<h2 id="Spark-实战案例"><a href="#Spark-实战案例" class="headerlink" title="Spark 实战案例"></a>Spark 实战案例</h2><h4 id="例⼦1，统计字符"><a href="#例⼦1，统计字符" class="headerlink" title="例⼦1，统计字符"></a>例⼦1，统计字符</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.wholeTextFiles(<span class="string">"/root/demo/src"</span>).map(x =&gt; <span class="keyword">for</span>(i &lt;-</span><br><span class="line">x._2.split(<span class="string">"\n"</span>))<span class="keyword">yield</span> i).flatMap(x=&gt;x).map(x =&gt; <span class="keyword">for</span>(i &lt;- x.split(<span class="string">"</span></span><br><span class="line"><span class="string">"</span>))<span class="keyword">yield</span> (i,<span class="number">1</span>) ).flatMap(x =&gt;</span><br><span class="line">x).reduceByKey(_+_).sortBy(_._2,<span class="literal">false</span>).collect</span><br></pre></td></tr></table></figure>
<h4 id="例⼦2，美国-1880-－-2014-年新⽣婴⼉数据统计"><a href="#例⼦2，美国-1880-－-2014-年新⽣婴⼉数据统计" class="headerlink" title="例⼦2，美国 1880 － 2014 年新⽣婴⼉数据统计"></a>例⼦2，美国 1880 － 2014 年新⽣婴⼉数据统计</h4><p><strong>下载数据</strong><br><a href="https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data" target="_blank" rel="noopener">https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data</a><br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark14.png" class="lazyload"><br><strong>代码模型</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.demo</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BabyCount</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">var</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"BabyCount"</span>)</span><br><span class="line">		<span class="keyword">var</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">		sc.wholeTextFiles(<span class="string">"/root/demo/src"</span>,<span class="number">40</span>)</span><br><span class="line">		.map((x)=&gt; <span class="keyword">for</span> (i &lt;- x._2.split(<span class="string">"\r\n"</span>)) <span class="keyword">yield</span>(x._1.substring(<span class="number">20</span>,</span><br><span class="line">		<span class="number">24</span>), i)).flatMap(x =&gt; x).map((x) =&gt; (x._1, x._2.split(<span class="string">","</span>)</span><br><span class="line">		(<span class="number">3</span>).toInt*x._2.split(<span class="string">","</span>)(<span class="number">2</span>).toDouble)).reduceByKey((x,y)=&gt;				(x+y),<span class="number">1</span>)</span><br><span class="line">		.map(x =&gt; (x._1+<span class="string">"\t"</span>+x._2)).saveAsTextFile(<span class="string">"/root/res"</span>)</span><br><span class="line">		sc.stop()</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>提交任务</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-submit --class</span><br><span class="line">com.baizhi.demo.BabyCount --executor-memory 512m --total-executor-cores 1</span><br><span class="line">--master spark://CentOS:7077 /root/babycount-1.0.jar</span><br></pre></td></tr></table></figure></p>
<h4 id="例⼦3，统计⽤户订单数据"><a href="#例⼦3，统计⽤户订单数据" class="headerlink" title="例⼦3，统计⽤户订单数据"></a>例⼦3，统计⽤户订单数据</h4><p><strong>模拟数据</strong><br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark15.png" class="lazyload"><br><strong>代码模型</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-shell --master</span><br><span class="line">spark://CentOS:7077 --executor-memory 512M --total-executor-cores 1</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span> sc.wholeTextFiles("/root/demo/src",40).map((x)=&gt; for (i &lt;-</span><br><span class="line">x._2.split("\n")) yield (i)).flatMap(x =&gt; x).map(x =&gt; (x.split(" ")(4) ,</span><br><span class="line">x.split(" ")(2).toDouble * x.split(" ")</span><br><span class="line">(3).toInt)).reduceByKey(_+_,1).sortBy(_._2,true).collect</span><br></pre></td></tr></table></figure></p>
<p><em><strong> IDEA开发 Spark</strong></em><br>1)  导⼊依赖<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.1<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">arg</span>&gt;</span>-dependencyfile<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;project.build.directory&#125;/.scala_dependen</span><br><span class="line">cies<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">								<span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">								<span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">									<span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">									<span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">									<span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">								<span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">							<span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">						<span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">					<span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">				<span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<p>2）设置IDEA的Scala的编译版本为scala-2.11版本<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark16.png" class="lazyload"></p>
<p>3) 编写wordcount代码实现计算<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordsCount"</span>);</span><br><span class="line"><span class="keyword">var</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">sc.textFile(args(<span class="number">0</span>)).flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">.map((_,<span class="number">1</span>)).reduceByKey(_+_).sortBy(_._2,<span class="literal">false</span>)</span><br><span class="line">.map(x=&gt;(x._1+<span class="string">"\t"</span>+x._2)).saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></p>
<p>4）调⽤spark-submit提交任务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-submit --master</span><br><span class="line">spark://CentOS:7077 --class com.baizhi.demo.WordCount --executor-memory 512m --total-executor-cores 1 /root/bb-1.0.jar /root/hello.txt /root/res</span><br></pre></td></tr></table></figure></p>
<p>5）访问主⻚⾯查看计算结果；<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark17.png" class="lazyload"></p>
  	
					
	  </div>     
	  

	
<div class="post-meta">
      
        <i class="iconfont icon-category"></i>       
        <a class="category-link" href="/categories/大数据/">大数据</a> 	
     
      	

      
        <i class="iconfont icon-tag"></i>     
          <a class="tag-link" href="/tags/BigData/">BigData</a> <a class="tag-link" href="/tags/Spark/">Spark</a>    
      	
</div>





<div class="post-footer">
  <div class="pf-left">
      <img class="pf-avatar lazyload" src="/images/placeholder.png" data-src="/images/header.jpg">
      <p class="pf-des">Hi, nice to meet you.</p>
  </div>

  <div class="pf-right">           
      <div class="pf-links">
        




<span class="donate-btn" title="赞赏">
	<span class="iconfont icon-donate"></span>
</span>


<div id="donate-box" class="sildeUpMin">

	<span class="donate-cancel iconfont icon-cancel"></span>

	<div class="donate-img-box">
		<img id="donate-qr-wechat" class="noLazyLoad donate-img lazyload" src="/images/placeholder.png" alt="No Donate Image!" data-src="/images/donate1.png">	
		<img id="donate-qr-alipay" class="noLazyLoad donate-img lazyload" src="/images/placeholder.png" alt="No Donate Image!" data-src="/images/donate2.png">	
	</div>

	<span class="donate-word">生活总是充满惊喜</span>

	<div class="donate-list">
		<span class="iconfont icon-donate-wechat" style="cursor:pointer"></span>&nbsp;&nbsp;
		<span class="iconfont icon-donate-alipay" style="cursor:pointer"></span>
	</div>

</div>

 
        
	
<script id="-mob-share" src="https://f1.webshare.mob.com/code/mob-share.js?appkey=266d31baa8b09"></script>
	
	<span class="share-btn" title="分享">
		<span class="iconfont icon-share"></span>
	</span>


	<div class="-mob-share sildeUpMin">
		   			             
            <a class="iconfont  icon-share-qq -mob-share-qq"></a>		
     	   			             
            <a class="iconfont  icon-share-weixin -mob-share-weixin"></a>		
     	   			             
            <a class="iconfont  icon-share-weibo -mob-share-weibo"></a>		
     	   			             
            <a class="iconfont  icon-share-douban -mob-share-douban"></a>		
     	   			             
            <a class="iconfont  icon-share-facebook -mob-share-facebook"></a>		
     	   			             
            <a class="iconfont  icon-share-twitter -mob-share-twitter"></a>		
     	   			             
            <a class="iconfont  icon-share-tumblr -mob-share-tumblr"></a>		
     	   			             
            <a class="iconfont  icon-share-google -mob-share-google"></a>		
     	   
	</div>	

      </div>  
    <nav class="pf-paginator">
           
        
      
        
        <a href="/2018/05/02/RocketMQ、Kafka和ActiveMQ对比分析/" data-hover="RocketMQ、Kafka和ActiveMQ对比分析"> 下一篇</a>
            
  </nav>   
  </div>
</div> 
	


    <div id="comment">   
    </div>

    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>

    <script src="//unpkg.com/valine@v1.1.8-beta/dist/Valine.min.js"></script>

    <script>
        new Valine({
        el: '#comment',
        notify:false, 
        verify:false, 
        app_id: 'qn6gri9c4QxH9F1Np1F2YJ5K-gzGzoHsz',
        app_key: '7H7or8956JshFahQvPgGg1yy',  
        path:window.location.pathname, 
        avatar:'mm' ,
        guest_info:['nick','mail']
        });
    </script>
	
</article>


	<div id="toc">		

		<div class="toc-container">	

		<span class="toc-contents"> 
			目录
		</span>

		<ul class="toc-list"></ul>

		</div>

	</div>

          </section> 
      </div>            
    
    <a id="backTop">
      <span>
        <i class="iconfont icon-backtotop"></i>
      </span>
    </a> 

  
    

        
        <div class="search-container sildeUpMin">


            <div class="search-header">
            <input type="text" placeholder="输入你想搜索的" id="search-input" class="search-input">  
            <span class="search-cancel iconfont icon-cancel"></span>
          
            </div>
              
            <div id="search-result" class="search-result"></div>

        </div>
 

     <div class="mobile-menu">      

      
      <img class="mobile-menu-icon lazyload" src="/images/placeholder.png" data-src="/images/favicon.png">   
      

         
            

            <a class="mobile-menu-link" href="/">首页
            </a>
            
         
            

            <a class="mobile-menu-link" href="/archives">归档
            </a>
            
         
            

            <a class="mobile-menu-link" href="/categories">分类
            </a>
            
         
            

            <a class="mobile-menu-link" href="/tags">标签
            </a>
            
         
            

            <a class="mobile-menu-link" href="/about">关于
            </a>
            
         
                          

            <a class="mobile-menu-link mobile-menu-search" href="#">搜索 </a>                 
            
         
      
</div>        
    



     
    




<footer id="footer">	    

		
		<div class="footer-copyright">
		&copy;
				
		2017-
		
		2018		
	
		&nbsp;CodingCow
		<br>

		Powered By
		<a href="https://hexo.io/" target="_blank">Hexo</a>
		<p>Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></p>
		</div>			
	 
</footer>   

  

    <script src="/nayo.bundle.js"></script>           
  </body>        
</html>