<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Spark深⼊浅出</title>
      <link href="/2018/05/27/Spark%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA/"/>
      <url>/2018/05/27/Spark%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA/</url>
      <content type="html"><![CDATA[<h1 id="～-gt-lt-～"><a href="#～-gt-lt-～" class="headerlink" title="～&gt;_&lt;～"></a>～&gt;_&lt;～</h1><h2 id="Spark-简介"><a href="#Spark-简介" class="headerlink" title="Spark 简介"></a>Spark 简介</h2><p>Apache Spark 是专为⼤规模数据处理⽽设计的快速通⽤的计算引擎。Spark是UC Berkeley AMP lab (加州⼤学伯克利分校AMP实验室)开源的类Hadoop MapReduce的通⽤并⾏框架，Spark拥有Hadoop MapReduce所具有并⾏计算的优点；但不同于MapReduce的是-Job中间输出结果可以保存在内存中，从⽽不再需要读写HDFS，因此Spark能更好地适⽤于数据挖掘与机器学习等需要迭代的MapReduce的算法。<br><img src="/images/spark/spark1.png" alt="img"><br><a id="more"></a></p><h4 id="Spark-发展历史"><a href="#Spark-发展历史" class="headerlink" title="Spark  发展历史"></a>Spark  发展历史</h4><p>对于⼀个具有相当技术⻔槛与复杂度的平台，Spark从诞⽣到正式版本的成熟，经历的时间如此之短，让⼈感到惊诧。2009年，Spark诞⽣于伯克利⼤学AMPLab，最开初属于伯克利⼤学的研究性项⽬。它于2010年正式开源，并于2013年成为了Aparch基⾦项⽬，并于2014年成为Aparch基⾦的顶级项⽬，整个过程不到五年时间。Apache Spark是专为⼤规模数据处理⽽设计的快速通⽤的计算引擎 ，现在形成⼀个⾼速发展应⽤⼴泛的⽣态系统。<br><img src="/images/spark/spark2.png" alt="img"><br>Spark提供的基于RDD的⼀体化解决⽅案，将MapReduce、Streaming、SQL、MachineLearning、Graph Processing等模型统⼀到⼀个平台下，并以⼀致的API公开，并提供相同的部署⽅案，使得Spark的⼯程应⽤领域变得更加⼴泛。Spark的特⾊在于它⾸先为⼤数据应⽤提供了⼀个统⼀的平台。从数据处理层⾯看，模型可以分为批处理、交互式、流处理等多种⽅式；⽽从⼤数据平台⽽⾔，已有成熟的Hadoop、Cassandra、Mesos以及其他云的供应商。</p><h4 id="Spark-amp-hadoop-关系"><a href="#Spark-amp-hadoop-关系" class="headerlink" title="Spark  &amp;  hadoop 关系"></a>Spark  &amp;  hadoop 关系</h4><p>Spark是对Map Reduce计算模型的改进，也可以理解为没有HDFS和MapReduce，就没有现在的Spark。 Spark可以使⽤YARN作为它的集群管理器，并且可以处理HDFS的数据。这对于已经部署Hadoop集群的⽤户特别重要，毕竟不需要做任何的数据迁移就可以使⽤Spark的强⼤处理能⼒。<br><img src="/images/spark/spark3.png" alt="img"></p><h4 id="Spark-计算-amp-Hadoop-MapReduce"><a href="#Spark-计算-amp-Hadoop-MapReduce" class="headerlink" title="Spark 计算&amp;Hadoop MapReduce"></a>Spark 计算&amp;Hadoop MapReduce</h4><p>Hadoop中Map Redcue由Map和Reduce两个阶段，并通过shuffle将两个阶段连接起来的。<br><img src="/images/spark/spark4.png" alt="img"><br>但是套⽤Map Reduce模型解决问题，不得不将问题分解为若⼲个有依赖关系的⼦问题，每个⼦问题对应⼀个Map Reduce作业，最终所有这些作业形成⼀个DAG。Spark是通⽤的DAG框架，可以将多个有依赖关系的作业转换为⼀个⼤的DAG。<br><img src="/images/spark/spark5.png" alt="img"><br>Spark核⼼思想是将Map和Reduce两个操作进⼀步拆分为多个元操作，这些元操作可以灵活组合，产⽣新的操作，并经过⼀些控制程序组装后形成⼀个⼤的DAG作业。由于Hadoop有多个MapReduce作业组成，每个作业都会从HDFS上读取⼀次数据和写⼀次数据（默认写三份），即使这些MapReduce作业产⽣的数据是中间数据也需要写HDFS。这种表达作业依赖关系的⽅式⽐较低效，会浪费⼤量不必要的磁盘和⽹络IO，根本原因是作业之间产⽣的数据不是直接流动的，⽽是借助HDFS作为共享数据存储系统。但是在Spark中，使⽤内存（内存不够使⽤本地磁盘）替代了使⽤HDFS存储中间结果。对于迭代运算效率更⾼。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark6.png" class="lazyload"></p><blockquote><p>注意：<font color="0099ff" face="黑体">中间结果</font>是指reduce操作后⽣成的结果，所以在⽐较Spark和Hadoop的计算模型的时候⼀般对⽐的是多个阶段的计算。</p></blockquote><h2 id="Spark-架构介绍"><a href="#Spark-架构介绍" class="headerlink" title="Spark 架构介绍"></a>Spark 架构介绍</h2><p>Spark的整体流程为：Client提交应⽤，Master找到⼀个Worker启动Driver，Driver向Master或者资源管理器申请资源，之后将应⽤转化为RDD Graph，再由DAGScheduler将RDD Graph转化为Stage的有向⽆环图提交给TaskScheduler，由TaskScheduler提交任务给Executor执⾏。在任务执⾏过程中，其他组件协同⼯作，确保整个应⽤顺利执⾏。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark7.png" class="lazyload"><br>Spark集群部署后，需要在主节点和从节点分别启动Master进程和Worker进程，对整个集群进⾏控制。在⼀个Spark应⽤的执⾏过程中，Driver和Worker是两个重要⻆⾊。Driver程序是应⽤逻辑执⾏的起点，负责作业的调度，即Task任务的分发，⽽多个Worker⽤来管理计算节点和创建Executor并⾏处理任务。在执⾏阶段，Driver会将Task和Task所依赖的file和jar包序列化后传递给对应的Worker机器，同时Executor对相应数据分区的任务进⾏处理。</p><h4 id="基本组件介绍"><a href="#基本组件介绍" class="headerlink" title="基本组件介绍"></a>基本组件介绍</h4><p><b>ClusterManager:</b> 在Standalone模式中即为Master(主节点)，控制整个集群，监控Worker。在YARN模式中为资源管理器。<br><b>Worker:</b> 从节点，负责控制计算节点，启动Executor或Driver。在YARN模式中为NodeManager，负责计算节点的控制。<br><b>Driver:</b> 运⾏Application的main()函数并创建SparkContext。<br><b>RDD:</b> Spark的基本运算单元，⼀组RDD可形成执⾏的有向⽆环图RDD Graph。<br><b>DAGScheduler:</b> 实现将Spark作业分解成⼀到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后⽣成相应的Task Set放到TaskScheduler中。<br><b>TaskScheduler:</b> 将任务(Task)分发给Executor执⾏。<br><b>Stag:</b>  ⼀个Spark作业⼀般包含⼀到多个Stage。<br><b>Task:</b> ⼀个Stage包含⼀到多个Task，通过多个Task实现并⾏运⾏的功能。</p><h2 id="Spark-安装部署"><a href="#Spark-安装部署" class="headerlink" title="Spark 安装部署"></a>Spark 安装部署</h2><p><em><strong>1. 安装 CentOS-6.5 64bit 环境，关闭防⽕墙</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS ~]# service iptables stop</span><br><span class="line">iptables: Setting chains to policy ACCEPT: filter [ OK ]</span><br><span class="line">iptables: Flushing firewall rules: [ OK ]</span><br><span class="line">iptables: Unloading modules: [ OK ]</span><br><span class="line">[root@CentOS ~]# chkconfig iptables off</span><br></pre></td></tr></table></figure></p><p><i><strong>2. 安装配置 jdk-8u171-linux-x64.rpm 配置 JAVA_HOME 环境变量（/root/.bashrc）</strong></i><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/java/latest</span><br><span class="line">CLASSPATH=.</span><br><span class="line">PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export JAVA_HOME</span><br><span class="line">export CLASSPATH</span><br><span class="line">export PATH</span><br><span class="line">[root@CentOS ~]# source .bashrc</span><br></pre></td></tr></table></figure></p><p><em><strong>3. 配置 IP 和主机名映射关系</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4</span><br><span class="line">localhost4.localdomain4</span><br><span class="line">::1 localhost localhost.localdomain localhost6</span><br><span class="line">localhost6.localdomain6</span><br><span class="line">192.168.80.128 CentOS</span><br></pre></td></tr></table></figure></p><p><em><strong>4. 下载 Spark 安装包<code>spark-2.3.0-bin-hadoop2.6.tgz</code>解压</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS ~]# tar -zxf spark-2.3.0-bin-hadoop2.6.tgz -C /usr</span><br></pre></td></tr></table></figure></p><h2 id="Spark-测试"><a href="#Spark-测试" class="headerlink" title="Spark 测试"></a>Spark 测试</h2><p><em><strong>1. 本地测试 Spark</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-shell --master</span><br><span class="line">local[5]</span><br></pre></td></tr></table></figure></p><p><em><strong>2. 启动 Spark 集群</strong></em><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./sbin/start-master.sh 【启动</span><br><span class="line">Master】</span><br><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./sbin/start-slave.sh --cores 2 --memory 512m spark://CentOS:7077 【启动work节点】</span><br><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-shell --master</span><br><span class="line">spark://CentOS:7077 --executor-memory 512M --total-executor-cores 1</span><br></pre></td></tr></table></figure></p><h2 id="RDD算⼦"><a href="#RDD算⼦" class="headerlink" title="RDD算⼦"></a>RDD算⼦</h2><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。RDD是Spark的最基本抽象,是对分布式内存的抽象使⽤，实现了以操作本地集合的⽅式来操作分布式数据集的抽象实现。<br>RDD是Spark最核⼼的东⻄，它表示已被分区、不可变的并能够被并⾏操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下⼀个操作可以直接从内存中输⼊，省去了MapReduce⼤量的磁盘IO操作。这对于迭代运算⽐较常⻅的机器学习算法, 交互式数据挖掘来说，效率提升⽐较⼤。</p><h4 id="RDD-特点"><a href="#RDD-特点" class="headerlink" title="RDD 特点"></a>RDD 特点</h4><h3 id="RDD计算流程"><a href="#RDD计算流程" class="headerlink" title="RDD计算流程"></a>RDD计算流程</h3><p>Spark的输⼊、运⾏转换、输出。在运⾏转换中通过算⼦对RDD进⾏转换。算⼦是RDD中定义的函数，可以对RDD中的数据进⾏转换和操作。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark8.png" class="lazyload"><br>1、输入：在Spark程序运行中，数据从外部数据空间（例如，HDFS、Scala集合或数据）输入到Spark，数据就进入了Spark运行时数据空间，会转化为Spark中的数据块，通过BlockManager进行管理。<br>2、运行：在Spark数据输入形成RDD后，便可以通过变换算子fliter等，对数据操作并将RDD转化为新的RDD，通过行动（Action）算子，触发Spark提交作业。如果数据需要复用，可以通过Cache算子，将数据缓存到内存。<br>3、输出：程序运行结束数据会输出Spark运行时空间，存储到分布式存储中（如saveAsTextFile输出到HDFS）或Scala数据或集合中（collect输出到Scala集合，count返回Scala Int型数据）。</p><h4 id="RDD编程模型"><a href="#RDD编程模型" class="headerlink" title="RDD编程模型"></a>RDD编程模型</h4><p>来看一段代码：textFile算子从HDFS读取日志文件，返回“file”（RDD）；filter算子筛出带 “ERROR” 的行，赋给 “errors”（新RDD）；cache算子把它缓存下来以备未来使用；count算子返回 “errors” 的行数。RDD看起来与Scala集合类型 没有太大差别，但它们的数据和运行模型大相迥异。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> file= sc.textFile(<span class="string">"hdfs://CentOS:9000/demo/src/access.log"</span>)</span><br><span class="line"><span class="keyword">var</span> errors = file.filter(_.contains(<span class="string">"ERROR"</span>))</span><br><span class="line">errors.cache()</span><br><span class="line">errors.count()</span><br></pre></td></tr></table></figure></p><p>上⾯代码给出了RDD数据模型，并将上例中⽤到的四个算⼦映射到四种算⼦类型。Spark程序⼯作在两个空间中：Spark RDD空间和Scala原⽣数据空间。在原⽣数据空间⾥，数据表现为标量（scalar，即Scala基本类型，⽤橘⾊⼩⽅块表示）、集合类型（蓝⾊虚线 框）和持久存储（红⾊圆柱）。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark9.png" class="lazyload"></p><h3 id="RDD运算逻辑"><a href="#RDD运算逻辑" class="headerlink" title="RDD运算逻辑"></a>RDD运算逻辑</h3><p>在Spark应用中，整个执行流程在逻辑上运算之间会形成有向无环图。Action算子触发之后会将所有累积的算子形成一个有向无环图，然后由调度器调度该图上的任务进行运算。Spark的调度方式与MapReduce有所不同。Spark根据RDD之间不同的依赖关系切分形成不同的阶段（Stage），一个阶段包含一系列函数进行流水线执行。图中的A、B、C、D、E、F、G，分别代表不同的RDD，RDD内的一个方框代表一个数据块（分区）。数据从HDFS输入Spark，形成RDD A和RDD C、RDD E，RDD C上执行map操作，转换为RDD D，RDD E和 RDD D 合并形成 RDD F，RDD B和RDD F进行join操作转换为G，而在B到G的过程中又会进行Shuffle。最后RDD G通过函数saveAsSequenceFile输出保存到HDFS中。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark10.png" class="lazyload"></p><h4 id="RDD依赖关系（narrow-wide）"><a href="#RDD依赖关系（narrow-wide）" class="headerlink" title="RDD依赖关系（narrow | wide）"></a>RDD依赖关系（narrow | wide）</h4><p>RDD间依赖分为 窄依赖 (narrow dependencies) 和宽依赖 (wide dependencies) 。窄依赖是指父 RDD 的每个分区都只被子RDD的一个分区所使用，例如map、filter。相应的，那么宽依赖就是指父 RDD 的分区被多个子 RDD 的分区所依赖，例如groupByKey、reduceByKey等操作。如果父RDD的一个Partition被一个子RDD的Partition所使用就是窄依赖，否则的话就是宽依赖。<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark11.png" class="lazyload"><br>窄依赖支持在一个结点上管道化执行。例如基于一对一的关系，可以在 filter 之后执行map 。其次，窄依赖支持更高效的故障还原。因为对于窄依赖，只有丢失的父 RDD 的分区需要重新计算。而对于宽依赖，一个结点的故障可能导致来自所有父 RDD 的分区丢失，因此就需要完全重新执行。因此对于宽依赖，Spark 会在持有各个父分区的结点上，将中间数据持久化来简化故障还原，就像 MapReduce 会持久化 map 的输出一样。Spark状态切换是遇到宽依赖就会生成一个State。</p><h3 id="RDD操作"><a href="#RDD操作" class="headerlink" title="RDD操作"></a>RDD操作</h3><p>1）创建RDD<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">scala&gt; sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">scala&gt; sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">45</span>),<span class="number">10</span>)</span><br><span class="line">scala&gt; sc.textFile(<span class="string">"/root/hello.txt"</span>)</span><br></pre></td></tr></table></figure></p><p>2）RDD算⼦（转换算⼦、Action算⼦）<br>转换算⼦<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark12.png" class="lazyload"><br>Action算⼦<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark13.png" class="lazyload"></p><p><strong>map、flatMap、filter⽤法</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>,<span class="number">2</span>,<span class="number">9</span>,<span class="number">1</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2 =sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>)).map(_*<span class="number">2</span>).sortBy(x=&gt;x,<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd2.filter(_&gt;<span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> rdd4 sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">3</span>,<span class="number">8</span>)).map(_*<span class="number">2</span>).sortBy(x=&gt;x+<span class="string">""</span>,<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> rdd5 =</span><br><span class="line">sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">10</span>)).map(_*<span class="number">2</span>).sortBy(x=&gt;x.toString,<span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> rdd6 = sc.parallelize(<span class="type">Array</span>(<span class="string">"a b c"</span>, <span class="string">"d e f"</span>, <span class="string">"h i j"</span>))</span><br><span class="line">rdd6.flatMap(_.split(' ')).collect</span><br></pre></td></tr></table></figure></p><p><strong>union求并集（类型要⼀致）</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd6 = sc.parallelize(<span class="type">List</span>(<span class="number">5</span>,<span class="number">6</span>,<span class="number">4</span>,<span class="number">7</span>))</span><br><span class="line"><span class="keyword">val</span> rdd7 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> rdd8 = rdd6.union(rdd7)</span><br><span class="line">rdd8.distinct.sortBy(x=&gt;x).collect</span><br></pre></td></tr></table></figure></p><p><strong>intersection求交集</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">1</span>), (<span class="string">"ls"</span>, <span class="number">2</span>), (<span class="string">"ww"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">9</span>), (<span class="string">"win7"</span>, <span class="number">8</span>), (<span class="string">"zl"</span>, <span class="number">7</span>)))</span><br><span class="line">rdd1.intersection(rdd2).collect</span><br></pre></td></tr></table></figure></p><p><strong>join操作</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">1</span>), (<span class="string">"ls"</span>, <span class="number">2</span>), (<span class="string">"ww"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">9</span>), (<span class="string">"win7"</span>, <span class="number">8</span>), (<span class="string">"zl"</span>, <span class="number">7</span>)))</span><br><span class="line">rdd1.join(rdd2).collect</span><br><span class="line">rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">rdd1.rightOuterJoin(rdd2).collect</span><br></pre></td></tr></table></figure></p><p><strong>groupByKey</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">1</span>), (<span class="string">"ls"</span>, <span class="number">2</span>), (<span class="string">"ww"</span>, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"zs"</span>, <span class="number">9</span>), (<span class="string">"win7"</span>, <span class="number">8</span>), (<span class="string">"zl"</span>, <span class="number">7</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1 union rdd2</span><br><span class="line">rdd3.groupByKey().map(x =&gt; (x._1,x._2.sum))</span><br></pre></td></tr></table></figure></p><p><strong>WordCount</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"/root/words.txt"</span>).flatMap(x=&gt;x.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).sortBy(_._2,<span class="literal">false</span>).collect</span><br><span class="line">sc.textFile(<span class="string">"/root/words.txt"</span>).flatMap(x=&gt;x.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).groupByKey.map(t=&gt;(t._1, t._2.sum)).collect</span><br></pre></td></tr></table></figure></p><p><strong>cogroup（相同key聚集在⼀起）</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>((<span class="string">"tom"</span>, <span class="number">1</span>), (<span class="string">"tom"</span>, <span class="number">2</span>), (<span class="string">"jerry"</span>, <span class="number">3</span>),(<span class="string">"kitty"</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>((<span class="string">"jerry"</span>, <span class="number">2</span>), (<span class="string">"tom"</span>, <span class="number">1</span>), (<span class="string">"shuke"</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.cogroup(rdd2)</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd3.map(t=&gt;(t._1, t._2._1.sum + t._2._2.sum))</span><br></pre></td></tr></table></figure></p><p><strong>di笛卡尔</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">List</span>(<span class="string">"tom"</span>, <span class="string">"jerry"</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">List</span>(<span class="string">"tom"</span>, <span class="string">"kitty"</span>, <span class="string">"shuke"</span>))</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.cartesian(rdd2)</span><br></pre></td></tr></table></figure></p><h2 id="Spark-实战案例"><a href="#Spark-实战案例" class="headerlink" title="Spark 实战案例"></a>Spark 实战案例</h2><h4 id="例⼦1，统计字符"><a href="#例⼦1，统计字符" class="headerlink" title="例⼦1，统计字符"></a>例⼦1，统计字符</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.wholeTextFiles(<span class="string">"/root/demo/src"</span>).map(x =&gt; <span class="keyword">for</span>(i &lt;-</span><br><span class="line">x._2.split(<span class="string">"\n"</span>))<span class="keyword">yield</span> i).flatMap(x=&gt;x).map(x =&gt; <span class="keyword">for</span>(i &lt;- x.split(<span class="string">"</span></span><br><span class="line"><span class="string">"</span>))<span class="keyword">yield</span> (i,<span class="number">1</span>) ).flatMap(x =&gt;</span><br><span class="line">x).reduceByKey(_+_).sortBy(_._2,<span class="literal">false</span>).collect</span><br></pre></td></tr></table></figure><h4 id="例⼦2，美国-1880-－-2014-年新⽣婴⼉数据统计"><a href="#例⼦2，美国-1880-－-2014-年新⽣婴⼉数据统计" class="headerlink" title="例⼦2，美国 1880 － 2014 年新⽣婴⼉数据统计"></a>例⼦2，美国 1880 － 2014 年新⽣婴⼉数据统计</h4><p><strong>下载数据</strong><br><a href="https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data" target="_blank" rel="noopener">https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data</a><br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark14.png" class="lazyload"><br><strong>代码模型</strong><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.demo</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BabyCount</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"><span class="keyword">var</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"BabyCount"</span>)</span><br><span class="line"><span class="keyword">var</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">sc.wholeTextFiles(<span class="string">"/root/demo/src"</span>,<span class="number">40</span>)</span><br><span class="line">.map((x)=&gt; <span class="keyword">for</span> (i &lt;- x._2.split(<span class="string">"\r\n"</span>)) <span class="keyword">yield</span>(x._1.substring(<span class="number">20</span>,</span><br><span class="line"><span class="number">24</span>), i)).flatMap(x =&gt; x).map((x) =&gt; (x._1, x._2.split(<span class="string">","</span>)</span><br><span class="line">(<span class="number">3</span>).toInt*x._2.split(<span class="string">","</span>)(<span class="number">2</span>).toDouble)).reduceByKey((x,y)=&gt;(x+y),<span class="number">1</span>)</span><br><span class="line">.map(x =&gt; (x._1+<span class="string">"\t"</span>+x._2)).saveAsTextFile(<span class="string">"/root/res"</span>)</span><br><span class="line">sc.stop()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>提交任务</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-submit --class</span><br><span class="line">com.baizhi.demo.BabyCount --executor-memory 512m --total-executor-cores 1</span><br><span class="line">--master spark://CentOS:7077 /root/babycount-1.0.jar</span><br></pre></td></tr></table></figure></p><h4 id="例⼦3，统计⽤户订单数据"><a href="#例⼦3，统计⽤户订单数据" class="headerlink" title="例⼦3，统计⽤户订单数据"></a>例⼦3，统计⽤户订单数据</h4><p><strong>模拟数据</strong><br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark15.png" class="lazyload"><br><strong>代码模型</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-shell --master</span><br><span class="line">spark://CentOS:7077 --executor-memory 512M --total-executor-cores 1</span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span> sc.wholeTextFiles("/root/demo/src",40).map((x)=&gt; for (i &lt;-</span><br><span class="line">x._2.split("\n")) yield (i)).flatMap(x =&gt; x).map(x =&gt; (x.split(" ")(4) ,</span><br><span class="line">x.split(" ")(2).toDouble * x.split(" ")</span><br><span class="line">(3).toInt)).reduceByKey(_+_,1).sortBy(_._2,true).collect</span><br></pre></td></tr></table></figure></p><p><em><strong> IDEA开发 Spark</strong></em><br>1)  导⼊依赖<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">maven.compiler.source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.source</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">maven.compiler.target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">maven.compiler.target</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.1<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.3.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">testSourceDirectory</span>&gt;</span>src/test/scala<span class="tag">&lt;/<span class="name">testSourceDirectory</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">arg</span>&gt;</span>-dependencyfile<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;project.build.directory&#125;/.scala_dependen</span><br><span class="line">cies<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>2）设置IDEA的Scala的编译版本为scala-2.11版本<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark16.png" class="lazyload"></p><p>3) 编写wordcount代码实现计算<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordsCount"</span>);</span><br><span class="line"><span class="keyword">var</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">sc.textFile(args(<span class="number">0</span>)).flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">.map((_,<span class="number">1</span>)).reduceByKey(_+_).sortBy(_._2,<span class="literal">false</span>)</span><br><span class="line">.map(x=&gt;(x._1+<span class="string">"\t"</span>+x._2)).saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></p><p>4）调⽤spark-submit提交任务<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@CentOS spark-2.3.0-bin-hadoop2.6]# ./bin/spark-submit --master</span><br><span class="line">spark://CentOS:7077 --class com.baizhi.demo.WordCount --executor-memory 512m --total-executor-cores 1 /root/bb-1.0.jar /root/hello.txt /root/res</span><br></pre></td></tr></table></figure></p><p>5）访问主⻚⾯查看计算结果；<br><img src="/images/placeholder.png" alt="img" data-src="/images/spark/spark17.png" class="lazyload"></p>]]></content>
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BigData </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>RocketMQ、Kafka和ActiveMQ对比分析</title>
      <link href="/2018/05/02/RocketMQ%E3%80%81Kafka%E5%92%8CActiveMQ%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/"/>
      <url>/2018/05/02/RocketMQ%E3%80%81Kafka%E5%92%8CActiveMQ%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<h1 id="（︶-︶）"><a href="#（︶-︶）" class="headerlink" title="（︶^︶）"></a>（︶^︶）</h1><h2 id="ActiveMQ"><a href="#ActiveMQ" class="headerlink" title="ActiveMQ"></a>ActiveMQ</h2><p>ActiveMQ作为传统型消息中间件，严格遵循JMS规范，功能全面，安装简单，需要的资源少。但是性能较差，不支持高并发和数以亿计的消息缓存，容错和扩展性差，不支持重复消费，不支持批处理。<br><a id="more"></a></p><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><h4 id="Kafka-的优点"><a href="#Kafka-的优点" class="headerlink" title="Kafka 的优点"></a>Kafka 的优点</h4><p><strong>1、消息持久化</strong><br>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。<br><strong>2、高性能 高吞吐率</strong><br>即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输，支持batch 操作。<br><strong>3、消息分区 分布式消费</strong><br>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。同时支持离线数据处理和实时数据处理（Kafka Stream）。<br><strong>4、分布式可扩展</strong><br>支持在线水平扩展（Scale out），Kafka 集群可以透明的扩展，增加新的服务器进集群。<br><strong>5、高容错</strong><br>Kafka每个Partition的数据都会复制到几台服务器上。当某个Broker故障失效时，ZooKeeper服务将通知生产者和消费者，生产者和消费者转而使用其它Broker。</p><h4 id="Kafka-的不足"><a href="#Kafka-的不足" class="headerlink" title="Kafka 的不足"></a>Kafka 的不足</h4><p><strong>1、重复消息</strong><br>Kafka 只保证每个消息至少会送达一次，虽然几率很小，但一条消息有可能会被送达多次。<br><strong>2、消息乱序</strong><br>虽然一个Partition 内部的消息是保证有序的，但是如果一个Topic 有多个Partition，Partition 之间的消息送达不保证有序。<br><strong>3、复杂性</strong><br>Kafka需要 zookeeper 集群的支持，Topic通常需要人工来创建，部署和维护较一般消息队列成本更高。</p><h4 id="相比RocketMQ，Kafka的不足之处还在于"><a href="#相比RocketMQ，Kafka的不足之处还在于" class="headerlink" title="相比RocketMQ，Kafka的不足之处还在于"></a>相比RocketMQ，Kafka的不足之处还在于</h4><p><strong>生产者发布消息的并行性受到分区数量的限制</strong><br>消费者消费并行度的程度也受到消费分区数量的限制。 假设分区数量为20，并发消费使用者的最大数量为20。每个主题由固定数量的分区组成。 分区号决定了单个代理在不明显影响性能的情况下可以拥有的最大主题数量。</p><blockquote><p>为什么Kafka无法支持更多的分区？<br>每个分区存储整个消息数据。 尽管每个分区都按顺序写入磁盘，但随着并发写入分区数量的增加，写入在操作系统的角度上变得随机。由于分散的数据文件，使得 Kafka 很难使用Linux IO组提交机制。</p></blockquote><blockquote><p>一般来说，Kafka集群中存在的分区越多，可以达到的吞吐量就越高，在某些情况下，分区太多也可能会产生负面影响。<br>（1）更多分区需要更多打开的文件句柄<br>（2）更多分区可能会增加不可用性<br>（3）更多分区可能会增加端到端延迟<br>（4）更多的分区可能需要更多的客户端内存<br><a href="https://www.confluent.io/blog/how-to-choose-the-number-of-topicspartitions-in-a-kafka-cluster/" target="_blank" rel="noopener">具体分析看这里</a></p></blockquote><h2 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="RocketMQ"></a>RocketMQ</h2><h4 id="为何RocketMQ能支持更多的分区？"><a href="#为何RocketMQ能支持更多的分区？" class="headerlink" title="为何RocketMQ能支持更多的分区？"></a>为何RocketMQ能支持更多的分区？</h4><p><img src="/images/kafka/rocketmq-queues.png" alt="img"><br>1、所有的消息数据都存储在提交日志文件中。所有写入都是完全顺序的，而读取是随机的。<br>2、ConsumeQueue存储实际用户消费位置信息，这些信息也以顺序方式刷新到磁盘。</p><blockquote><p>优点：</p><ol><li>每个消费队列都是轻量级的，并且包含有限的元数据。</li><li>对磁盘的访问是完全顺序的，这可以避免磁盘锁争用，并且在创建大量队列时不会导致高磁盘IO等待。</li></ol></blockquote><blockquote><p>缺点：</p><ol><li>消息消耗将首先读取消耗队列，然后提交日志。这个过程在最坏的情况下会带来一定的成本。</li><li>提交日志和消耗队列需要在逻辑上一致，这给编程模型带来了额外的复杂性。</li></ol></blockquote><blockquote><p>设计动机：</p><ol><li>随机阅读。尽可能多地读取以增加页面缓存命中率，并减少读取IO操作。所以大的内存仍然是可取的。如果大量消息累积，读取性能会严重下降吗？答案是否定的，原因如下：<br>(1) 即使消息的大小仅为1KB，系统也会提前读取更多数据，请参阅PAGECACHE预取以供参考。这意味着对于后续数据读取，它将访问将要执行的主存储器，而不是慢速磁盘IO读取。<br>(2) 从磁盘随机访问提交日志。如果在SSD的情况下将 I/O 调度器设置为NOOP，则读取的QPS将大大加速，因此比其他电梯调度器算法快得多。</li><li>由于ConsumeQueue只存储固定大小的元数据，主要用于记录消费进度，所以随机读取得到很好的支持。利用页面缓存（PageCache）预取功能，访问ConsumeQueue的速度与访问主内存一样快，即使是在大量消息积累的情况下。因此，ConsumeQueue不会对阅读性能造成显着的损失。</li><li>CommitLog存储几乎所有的信息，包括消息数据。与关系数据库的重做日志类似，只要提交日志存在，消费队列，消息密钥索引和所有其他所需数据都可以完全恢复。</li></ol></blockquote><h2 id="RocketMQ-vs-ActiveMQ-vs-Kafka"><a href="#RocketMQ-vs-ActiveMQ-vs-Kafka" class="headerlink" title="RocketMQ vs. ActiveMQ vs. Kafka"></a>RocketMQ vs. ActiveMQ vs. Kafka</h2><p><img src="/images/kafka/mqvs.png" alt="img"></p>]]></content>
      
      <categories>
          
          <category> 消息队列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MQ </tag>
            
            <tag> Kafka </tag>
            
            <tag> RocketMQ </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Solr轻松入门</title>
      <link href="/2018/04/27/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Solr%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8/"/>
      <url>/2018/04/27/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Solr%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp;</p>]]></content>
      
      <categories>
          
          <category> 全文检索 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Solr </tag>
            
            <tag> Index </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Dubbo原理分析</title>
      <link href="/2018/04/27/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Dubbo%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"/>
      <url>/2018/04/27/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Dubbo%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/</url>
      <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp;</p>]]></content>
      
      <categories>
          
          <category> 中间件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dubbo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SpringBoot集成MongoDB</title>
      <link href="/2018/03/09/SpringBoot%E9%9B%86%E6%88%90MongoDB/"/>
      <url>/2018/03/09/SpringBoot%E9%9B%86%E6%88%90MongoDB/</url>
      <content type="html"><![CDATA[<p><img src="/images/springboot_mongo.jpg" alt="Alt text"></p><h1 id="（-gt-﹏-lt-）"><a href="#（-gt-﹏-lt-）" class="headerlink" title="（&gt;﹏&lt;）"></a>（&gt;﹏&lt;）</h1><p>&nbsp; &nbsp; &nbsp; &nbsp; 最近在做日志采集的工作中用到了MongoDB这款NoSQL数据库，相较于常见的RDBMS灵活且快速；而且非结构化的数据存储格式也极为适合一些应用场景，既节省内存又能提高读写速度。了解如何将MongoDB集成到SpringBoot这样一款非常流行的轻量、快速的开发框架很有必要。<br><a id="more"></a></p><h2 id="创建Spring-Boot工程并导入Maven依赖"><a href="#创建Spring-Boot工程并导入Maven依赖" class="headerlink" title="创建Spring Boot工程并导入Maven依赖"></a>创建Spring Boot工程并导入Maven依赖</h2><blockquote><p>注：示例代码使用的Spring Boot版本为：2.0.1.RELEASE  </p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-data-mongodb<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="修改配置文件-application-yml"><a href="#修改配置文件-application-yml" class="headerlink" title="修改配置文件 application.yml"></a>修改配置文件 application.yml</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line"><span class="attr">  data:</span></span><br><span class="line"><span class="attr">    mongodb:</span></span><br><span class="line"><span class="attr">      host:</span> <span class="number">192.168</span><span class="number">.109</span><span class="number">.128</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">27017</span></span><br><span class="line"><span class="attr">      database:</span> <span class="string">test</span></span><br></pre></td></tr></table></figure><h2 id="创建Employee实体"><a href="#创建Employee实体" class="headerlink" title="创建Employee实体"></a>创建Employee实体</h2><blockquote><p>注：Employee 映射 Document  </p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Document</span>(collection = <span class="string">"employee"</span>) <span class="comment">// 集合名</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Id</span></span><br><span class="line">    <span class="keyword">private</span> String id;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Double salary;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Date birthday;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 省略 getter/setter</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="创建MongoRepository"><a href="#创建MongoRepository" class="headerlink" title="创建MongoRepository"></a>创建MongoRepository</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.orchoe.repository;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.orchoe.entity.Employee;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.data.mongodb.core.MongoTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.data.mongodb.core.query.Criteria;</span><br><span class="line"><span class="keyword">import</span> org.springframework.data.mongodb.core.query.Query;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Repository;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Austin</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2018/3/07 16:42</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Repository</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MongoRepository</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> MongoTemplate mongoTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insertOne</span><span class="params">(Employee employee)</span></span>&#123;</span><br><span class="line">        mongoTemplate.insert(employee);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteOne</span><span class="params">(Employee employee)</span></span>&#123;</span><br><span class="line">        mongoTemplate.remove(employee);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> User <span class="title">findOne</span><span class="params">(String id)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> mongoTemplate.findById(id,Employee.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Employee&gt; <span class="title">findAll</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> mongoTemplate.findAll(Employee.class);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Employee&gt; <span class="title">findByName</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> mongoTemplate.find(Query.query(Criteria.where(<span class="string">"name"</span>).is(name)),Employee.class);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="创建测试类"><a href="#创建测试类" class="headerlink" title="创建测试类"></a>创建测试类</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.orchoe.test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.orchoe.repository.MongoRepository;</span><br><span class="line"><span class="keyword">import</span> com.orchoe.entity.Employee;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"><span class="keyword">import</span> org.junit.runner.RunWith;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class="line"><span class="keyword">import</span> org.springframework.test.context.junit4.SpringRunner;</span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="meta">@RunWith</span>(SpringRunner.class)</span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SpringbootMongodbApplicationTests</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Autowired</span></span><br><span class="line">   <span class="keyword">private</span> MongoRepository mongoRepository;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@Test</span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">contextLoads</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        mongoRepository.insertOne(<span class="keyword">new</span> Employee(<span class="string">"zhangsan"</span>, <span class="number">2000.0</span>D, <span class="keyword">new</span> Date()));</span><br><span class="line">        List&lt;Employee&gt; employees = mongoRepository.findAll();</span><br><span class="line">        employees.forEach(employee -&gt; System.out.println(employee));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> 教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MongoDB </tag>
            
            <tag> SpringBoot </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Shiro轻松入门</title>
      <link href="/2018/03/06/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Shiro%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8/"/>
      <url>/2018/03/06/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Shiro%E8%BD%BB%E6%9D%BE%E5%85%A5%E9%97%A8/</url>
      <content type="html"><![CDATA[<h1 id="“▔□▔"><a href="#“▔□▔" class="headerlink" title="(“▔□▔)/"></a>(“▔□▔)/</h1><p>## </p><p>##</p><p>##</p>]]></content>
      
      <categories>
          
          <category> 权限管理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shiro </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MongoDB的优点和缺点</title>
      <link href="/2018/02/22/MongoDB%E7%9A%84%E4%BC%98%E7%82%B9%E5%92%8C%E7%BC%BA%E7%82%B9/"/>
      <url>/2018/02/22/MongoDB%E7%9A%84%E4%BC%98%E7%82%B9%E5%92%8C%E7%BC%BA%E7%82%B9/</url>
      <content type="html"><![CDATA[<h1 id="≡￣﹏￣≡"><a href="#≡￣﹏￣≡" class="headerlink" title="≡￣﹏￣≡"></a>≡￣﹏￣≡</h1><h2 id="与关系型数据库相比，MongoDB的优点："><a href="#与关系型数据库相比，MongoDB的优点：" class="headerlink" title="与关系型数据库相比，MongoDB的优点："></a>与关系型数据库相比，MongoDB的优点：</h2><p>① 弱一致性（最终一致），更能保证用户的访问速度。</p><p>② 文档结构的存储方式（类JSON数据模式简单而强大），能够更便捷的获取数据。<br><a id="more"></a><br>③ 全索引支持,扩展到内部对象和内嵌数组。</p><p>④ 内置GridFS，支持大容量的存储。<br>  GridFS是一个出色的分布式文件系统，可以支持海量的数据存储。<br>  内置了GridFS了MongoDB，能够满足对大数据集的快速范围查询。</p><p>⑤ 内置Sharding。<br>提供基于Range的Auto Sharding机制：一个collection可按照记录的范围，分成若干个段，切分到不同的Shard上。</p><p>⑥ 第三方支持丰富。(这是与其他的NoSQL相比，MongoDB也具有的优势)<br>现开源文档数据库MongoDB背后有商业公司10gen为其提供供商业培训和支持，而且MongoDB社区非常活跃，很多开发框架都迅速提供了对MongDB的支持。</p><p>⑦ 性能优越：<br>在使用场合下，千万级别的文档对象，近10G的数据，对有索引的ID的查询不会比mysql慢，而对非索引字段的查询，则是全面胜出。</p><h2 id="与关系型数据库相比，MongoDB的缺点："><a href="#与关系型数据库相比，MongoDB的缺点：" class="headerlink" title="与关系型数据库相比，MongoDB的缺点："></a>与关系型数据库相比，MongoDB的缺点：</h2><p>① mongodb不支持事务操作，所以事务要求严格的系统（如果银行系统）肯定不能用它。(这点和优点①是对应的)</p><p>② mongodb占用空间过大。</p><blockquote><p>关于其原因，在官方的FAQ中，提到有如下几个方面：<br>1、空间的预分配：为避免形成过多的硬盘碎片，mongodb每次空间不足时都会申请生成一大块的硬盘空间，而且申请的量从64M、128M、256M那样的指数递增，直到2G为单个文件的最大体积。随着数据量的增加，你可以在其数据目录里看到这些整块生成容量不断递增的文件。</p><p>2、字段名所占用的空间：为了保持每个记录内的结构信息用于查询，mongodb需要把每个字段的key-value都以BSON的形式存储，如果 value域相对于key域并不大，比如存放数值型的数据，则数据的overhead是最大的。一种减少空间占用的方法是把字段名尽量取短一些，这样占用空间就小了，但这就要求在易读性与空间占用上作为权衡了。我曾建议作者把字段名作个index，每个字段名用一个字节表示，这样就不用担心字段名取多长了。但作者的担忧也不无道理，这种索引方式需要每次查询得到结果后把索引值跟原值作一个替换，再发送到客户端，这个替换也是挺耗费时间的。现在的实现算是拿空间来换取时间吧。</p><p>3、删除记录不释放空间：这很容易理解，为避免记录删除后的数据的大规模挪动，原记录空间不删除，只标记“已删除”即可，以后还可以重复利用。</p><p>4、可以定期运行db.repairDatabase()来整理记录，但这个过程会比较缓慢</p></blockquote><p>③ MongoDB没有如MySQL那样成熟的维护工具，这对于开发和IT运营都是个值得注意的地方。</p>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MongoDB </tag>
            
            <tag> NoSQL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Redis解决分布式会话问题</title>
      <link href="/2018/02/07/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Redis%E8%A7%A3%E5%86%B3%E5%88%86%E5%B8%83%E5%BC%8F%E4%BC%9A%E8%AF%9D%E9%97%AE%E9%A2%98/"/>
      <url>/2018/02/07/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Redis%E8%A7%A3%E5%86%B3%E5%88%86%E5%B8%83%E5%BC%8F%E4%BC%9A%E8%AF%9D%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp;</p>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NoSQL </tag>
            
            <tag> Redis </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Lucene使用技巧总结</title>
      <link href="/2018/01/22/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Lucene%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/"/>
      <url>/2018/01/22/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/Lucene%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp;</p>]]></content>
      
      <categories>
          
          <category> 全文检索 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Lucene </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>二叉查找树和红黑树</title>
      <link href="/2017/12/21/%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91/"/>
      <url>/2017/12/21/%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91%E5%92%8C%E7%BA%A2%E9%BB%91%E6%A0%91/</url>
      <content type="html"><![CDATA[<h1 id="┐（─-─）┌"><a href="#┐（─-─）┌" class="headerlink" title="┐（─__─）┌"></a>┐（─__─）┌</h1><h2 id="二叉查找树（BST）的特性"><a href="#二叉查找树（BST）的特性" class="headerlink" title="二叉查找树（BST）的特性"></a>二叉查找树（BST）的特性</h2><p>(1) 左子树上所有结点的值均小于或等于它的根结点的值。</p><p>(2) 右子树上所有结点的值均大于或等于它的根结点的值。</p><p>(3) 左、右子树也分别为二叉排序树。</p><p>下图中这棵树，就是一颗典型的二叉查找树：<br><a id="more"></a><br><img src="/images/redBlackTree/binarytree.jpeg" alt="Alt BST"><br>如果要查找图中值为10的节点：<br><img src="/images/redBlackTree/binarytree.png" alt="Alt BST"><br>顺序为：先查看根节点9；由于10 &gt; 9，因此查看右孩子13；由于10 &lt; 13，因此查看左孩子11；由于10 &lt; 11，因此查看左孩子10，发现10正是要查找的节点。</p><p>这种方式正是二分查找的思想，查找所需的最大次数等同于二叉查找树的高度。在插入节点的时候也是利用类似的方法，通过一层一层比较大小找到新节点适合插入的位置。</p><h2 id="二叉查找树的缺陷"><a href="#二叉查找树的缺陷" class="headerlink" title="二叉查找树的缺陷"></a>二叉查找树的缺陷</h2><p>二叉查找树在多次插入新节点之后可能树结构的不平衡，形成线性结构，降低查找性能。<br>例如，假设初始的二叉查找树只有三个节点，根节点值为9，左孩子值为8，右孩子值为12：<br><img src="/images/redBlackTree/bst1.png" alt="Alt BST"><br>接下来我们依次插入如下五个节点：7,6,5,4,3。依照二叉查找树的特性，结果会变成什么样呢？<br><img src="/images/redBlackTree/bst2.png" alt="Alt BST"><br>为解决这一缺陷，于是……</p><h2 id="红黑树诞生"><a href="#红黑树诞生" class="headerlink" title="红黑树诞生"></a>红黑树诞生</h2><p>红黑树（Red Black Tree）是一种自平衡的二叉查找树，除了符合二叉查找树的基本特性外，它还具有下列的附加特性：<br>①  节点是红色或黑色。<br>②  根节点是黑色。<br>③  每个叶子节点都是黑色的空节点（NIL节点）。<br>④  每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)<br>⑤  从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。</p><p>下图中这棵树，就是一颗典型的红黑树：<br><img src="/images/redBlackTree/rbt1.jpeg" alt="Alt BST"></p><blockquote><p>红黑树从跟到叶子的最长路径不会超过最短路径的2倍。<br>在插入或删除节点时红黑树的规则可能被打破，这就需要旋转或变色来维持我们的规则。</p></blockquote><p>什么情况下会破坏红黑树的规则，什么情况下不会破坏规则呢？我们举两个简单的栗子：</p><p><strong>栗子1：不会破坏规则</strong><br>向原红黑树插入值为14的新节点：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt2.jpeg" class="lazyload"><br><strong>栗子2：会破坏规则</strong><br>向原红黑树插入值为21的新节点：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt3.jpeg" class="lazyload"><br>由于父节点22是红色节点，因此这种情况打破了红黑树的规则4（每个红色节点的两个子节点都是黑色），必须进行调整，使之重新符合红黑树的规则。调整有两种方法<font color="blue">[ 变色 ]</font>和<font color="blue">[ 旋转 ]</font>；旋转又分为[ 左旋转 ]和[ 右旋转 ]。</p><h3 id="变色："><a href="#变色：" class="headerlink" title="变色："></a>变色：</h3><p>为了重新符合红黑树的规则，尝试把红色节点变为黑色，或者把黑色节点变为红色。</p><p>下图所表示的是红黑树的一部分，需要注意节点25并非根节点。因为节点21和节点22连续出现了红色，不符合规则4，所以把节点22从红色变成黑色：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt4.jpeg" class="lazyload"><br>但这样并不算完，因为凭空多出的黑色节点打破了规则5，所以发生连锁反应，需要继续把节点25从黑色变成红色：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt5.jpeg" class="lazyload"><br>此时仍然没有结束，因为节点25和节点27又形成了两个连续的红色节点，需要继续把节点27从红色变成黑色：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt6.jpeg" class="lazyload"></p><h3 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h3><blockquote><p><strong>左旋转：</strong><br>逆时针旋转红黑树的两个节点，使得父节点被自己的右孩子取代，而自己成为自己的左孩子。说起来很怪异，大家看下图：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/lrotate.png" class="lazyload"><br>图中，身为右孩子的Y取代了X的位置，而X变成了自己的左孩子。此为左旋转。</p></blockquote><blockquote><p><strong>右旋转：</strong><br>顺时针旋转红黑树的两个节点，使得父节点被自己的左孩子取代，而自己成为自己的右孩子。大家看下图：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rrotate.png" class="lazyload"><br>图中，身为左孩子的Y取代了X的位置，而X变成了自己的右孩子。此为右旋转。</p></blockquote><p>我们以刚才插入节点21的情况为例：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt7.jpeg" class="lazyload"><br>首先，我们需要做的是变色，把节点25及其下方的节点变色：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt8.jpeg" class="lazyload"><br>此时节点17和节点25是连续的两个红色节点，那么把节点17变成黑色节点？恐怕不合适。这样一来不但打破了规则4，而且根据规则2（根节点是黑色），也不可能把节点13变成红色节点。</p><p>变色已无法解决问题，我们把节点13看做X，把节点17看做Y，像刚才的示意图那样进行左旋转：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/lrotate.png" class="lazyload"><br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt9.jpeg" class="lazyload"><br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt10.jpeg" class="lazyload"><br>由于根节点必须是黑色节点，所以需要变色，变色结果如下：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt11.jpeg" class="lazyload"><br>这样就结束了吗？并没有。因为其中两条路径(17 -&gt; 8 -&gt; 6 -&gt; NIL)的黑色节点个数是4，其他路径的黑色节点个数是3，不符合规则5。</p><p>这时候我们需要把节点13看做X，节点8看做Y，像刚才的示意图那样进行右旋转：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rrotate.png" class="lazyload"><br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt12.jpeg" class="lazyload"><br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt13.jpeg" class="lazyload"><br>最后根据规则来进行变色：<br><img src="/images/placeholder.png" alt="Alt BST" data-src="/images/redBlackTree/rbt14.jpeg" class="lazyload"><br>如此一来，我们的红黑树变得重新符合规则。这一个例子的调整过程比较复杂，经历了如下步骤：</p><p>变色 -&gt; 左旋转 -&gt; 变色 -&gt; 右旋转 -&gt; 变色</p><blockquote><p>红黑树的应用有很多，其中 JDK 的集合类 TreeSet 和 TreeMap 底层就是用红黑树实现的。另外Java 8 中的 HashMap 也用到了红黑树。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tree </tag>
            
            <tag> Binary Tree </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>HashMap原理分析(JDK1.8)</title>
      <link href="/2017/12/21/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/HashMap%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90(JDK1.8)/"/>
      <url>/2017/12/21/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/HashMap%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90(JDK1.8)/</url>
      <content type="html"><![CDATA[<p>&nbsp; &nbsp; &nbsp; &nbsp; HashMap是基于哈希表的Map接口的非同步实现。此实现提供所有可选的映射操作，并允许使用null值和null键。HashMap类不保证映射的顺序，特别是它不保证该顺序恒久不变。</p><h2 id="HashMap的数据结构"><a href="#HashMap的数据结构" class="headerlink" title="HashMap的数据结构"></a>HashMap的数据结构</h2><p>&nbsp; &nbsp; &nbsp; &nbsp; 在Java编程语言中，最基本的结构就是两种，一个是数组，另外一个是模拟指针（引用），所有的数据结构都可以用这两个基本结构来构造的，HashMap也不例外。HashMap实际上是一个“链表散列”的数据结构，即数组和链表的结合体。<br><img src="/images/hashmap.png" alt="Alt text"><br>&nbsp; &nbsp; &nbsp; &nbsp; 从上图中可以看出，HashMap底层就是一个数组结构，数组中的每一项又是一个链表。当新建一个HashMap的时候，就会初始化一个数组。<br><a id="more"></a><br>我们通过JDK中的HashMap源码进行一些学习，首先看一下构造函数：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">(<span class="keyword">int</span> initialCapacity, <span class="keyword">float</span> loadFactor)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal initial capacity: "</span> + initialCapacity);</span><br><span class="line">        <span class="keyword">if</span> (initialCapacity &gt; MAXIMUM_CAPACITY)</span><br><span class="line">            initialCapacity = MAXIMUM_CAPACITY;</span><br><span class="line">        <span class="keyword">if</span> (loadFactor &lt;= <span class="number">0</span> || Float.isNaN(loadFactor))</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Illegal load factor: "</span> +loadFactor);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Find a power of 2 &gt;= initialCapacity</span></span><br><span class="line">        <span class="keyword">int</span> capacity = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (capacity &lt; initialCapacity)</span><br><span class="line">            capacity &lt;&lt;= <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.loadFactor = loadFactor;</span><br><span class="line">        threshold = (<span class="keyword">int</span>)Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + <span class="number">1</span>);</span><br><span class="line">        table = <span class="keyword">new</span> Entry[capacity];</span><br><span class="line">        useAltHashing = sun.misc.VM.isBooted() &amp;&amp;</span><br><span class="line">                (capacity &gt;= Holder.ALTERNATIVE_HASHING_THRESHOLD);</span><br><span class="line">        init();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>我们着重看一下第18行代码table = new Entry[capacity];。这不就是Java中数组的创建方式吗？也就是说在构造函数中，其创建了一个Entry的数组，其大小为capacity（目前我们还不需要太了解该变量含义），那么Entry又是什么结构呢？看一下源码：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* The table, resized as necessary. Length MUST Always be a power of two.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">transient</span> Entry[] table;</span><br><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"><span class="keyword">final</span> K key;</span><br><span class="line">V value;</span><br><span class="line">Entry&lt;K,V&gt; next;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> hash;</span><br><span class="line">……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Core Java基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HashMap </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MySQL中的几种引擎介绍</title>
      <link href="/2017/12/18/MySQL%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8D%E5%BC%95%E6%93%8E%E4%BB%8B%E7%BB%8D/"/>
      <url>/2017/12/18/MySQL%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8D%E5%BC%95%E6%93%8E%E4%BB%8B%E7%BB%8D/</url>
      <content type="html"><![CDATA[<h1 id="（￣∞￣）"><a href="#（￣∞￣）" class="headerlink" title="（￣∞￣）"></a>（￣∞￣）</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>数据库存储引擎是数据库底层软件组织，数据库管理系统（DBMS）使用数据引擎进行创建、查询、更新和删除数据。不同的存储引擎提供不同的存储机制、索引技巧、锁定水平等功能，使用不同的存储引擎，还可以获得特定的功能。现在许多不同的数据库管理系统都支持多种不同的数据引擎。<strong>MySql的核心就是存储引擎</strong>。</p><p><strong>存储引擎查看</strong></p><p>MySQL给开发者提供了查询存储引擎的功能，我这里使用的是MySQL5.5，可以使用：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">ENGINES</span></span><br></pre></td></tr></table></figure></p><a id="more"></a><p>命令来查看MySQL使用的引擎，命令的输出为（我用的Navicat Premium）：<br><img src="/images/mysql/storageEngine1.png" alt="img"><br>MySQL给用户提供了多种存储引擎，包括处理事务安全表的引擎和处理非事务安全表的引擎。</p><p>如果要想查看数据库默认使用哪个引擎，可以通过使用命令：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">VARIABLES</span> <span class="keyword">LIKE</span> <span class="string">'storage_engine'</span>;</span><br></pre></td></tr></table></figure></p><p>来查看，查询结果为：<br><img src="/images/mysql/storageEngine2.png" alt="img"></p><h2 id="修改数据库引擎："><a href="#修改数据库引擎：" class="headerlink" title="修改数据库引擎："></a>修改数据库引擎：</h2><p>方式一：<br>修改配置文件my.ini<br>将mysql.ini另存为my.ini，在[mysqld]后面添加default-storage-engine=InnoDB，重启服务，数据库默认的引擎修改为InnoDB<br>方式二：<br>在建表的时候指定<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> mytbl(   </span><br><span class="line">   <span class="keyword">id</span> <span class="built_in">int</span> primary <span class="keyword">key</span>,   </span><br><span class="line">   <span class="keyword">name</span> <span class="built_in">varchar</span>(<span class="number">50</span>)   </span><br><span class="line">)<span class="keyword">type</span>=MyISAM;</span><br></pre></td></tr></table></figure></p><p>方式三：<br>建表后更改<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">type</span> = <span class="keyword">InnoDB</span>;</span><br></pre></td></tr></table></figure></p><p><strong>怎么查看修改成功</strong><br>方式一：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">table</span> <span class="keyword">status</span> <span class="keyword">from</span> table_name;</span><br></pre></td></tr></table></figure></p><p>方式二：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> table_name</span><br></pre></td></tr></table></figure></p><p>方式三：<br>使用数据库管理工具啊。</p><p>在MySQL中，不需要在整个服务器中使用同一种存储引擎，针对具体的要求，可以对每一个表使用不同的存储引擎。Support列的值表示某种引擎是否能使用：YES表示可以使用、NO表示不能使用、DEFAULT表示该引擎为当前默认的存储引擎 。下面来看一下其中几种常用的引擎。</p><h2 id="InnoDB："><a href="#InnoDB：" class="headerlink" title="InnoDB："></a>InnoDB：</h2><p>定义：（默认的存储引擎）<br>InnoDB是一个事务型的存储引擎，有行级锁定和外键约束。<br>Innodb引擎提供了对数据库ACID（原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability））事务的支持，并且实现了SQL标准的四种隔离级别，关于数据库事务与其隔离级别的内容请见数据库事务与其隔离级别这类型的文章。该引擎还提供了行级锁和外键约束，它的设计目标是处理大容量数据库系统，它本身其实就是基于MySQL后台的完整数据库系统，MySQL运行时Innodb会在内存中建立缓冲池，用于缓冲数据和索引。但是该引擎不支持FULLTEXT类型的索引，而且它没有保存表的行数，当SELECT COUNT(*) FROM TABLE时需要扫描全表。当需要使用数据库事务时，该引擎当然是首选。由于锁的粒度更小，写操作不会锁定全表，所以在并发较高时，使用Innodb引擎会提升效率。但是使用行级锁也不是绝对的，如果在执行一个SQL语句时MySQL不能确定要扫描的范围，InnoDB表同样会锁全表。<br>//这个就是select锁表的一种，不明确主键。增删改查都可能会导致锁全表，在以后我们会详细列出。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> products <span class="keyword">WHERE</span> <span class="keyword">name</span>=<span class="string">'Mouse'</span> <span class="keyword">FOR</span> <span class="keyword">UPDATE</span>;</span><br></pre></td></tr></table></figure></p><p>适用场景：<br>1）经常更新的表，适合处理多重并发的更新请求。<br>2）支持事务。<br>3）可以从灾难中恢复（通过bin-log日志等）。<br>4）外键约束。只有他支持外键。<br>5）支持自动增加列属性auto_increment。</p><p>MySQL官方对 InnoDB 的讲解：<br>1）InnoDB给MySQL提供了具有提交、回滚和崩溃恢复能力的事务安全（ACID兼容）存储引擎。<br>2）InnoDB锁定在行级并且也在SELECT语句提供一个Oracle风格一致的非锁定读，这些特色增加了多用户部署和性能。没有在InnoDB中扩大锁定的需要，因为在InnoDB中行级锁定适合非常小的空间。<br>3）InnoDB也支持FOREIGN KEY强制。在SQL查询中，你可以自由地将InnoDB类型的表与其它MySQL的表的类型混合起来，甚至在同一个查询中也可以混合。<br>4）InnoDB是为处理巨大数据量时的最大性能设计，它的CPU效率可能是任何其它基于磁盘的关系数据库引擎所不能匹敌的。<br>5） InnoDB被用来在众多需要高性能的大型数据库站点上产生。</p><h2 id="MyISAM："><a href="#MyISAM：" class="headerlink" title="MyISAM："></a>MyISAM：</h2><p>定义：<br>MyIASM是MySQL默认的引擎，但是它没有提供对数据库事务的支持，也不支持行级锁和外键，因此当INSERT(插入)或UPDATE(更新)数据时即写操作需要锁定整个表，效率便会低一些。<br>MyISAM 存储引擎独立于操作系统，也就是可以在windows上使用，也可以比较简单的将数据转移到linux操作系统上去。<br>意味着：引擎在创建表的时候，会创建三个文件，一个是.frm文件用于存储表的定义，一个是.MYD文件用于存储表的数据，另一个是.MYI文件，存储的是索引。操作系统对大文件的操作是比较慢的，这样将表分为三个文件，那么.MYD这个文件单独来存放数据自然可以优化数据库的查询等操作。有索引管理和字段管理。MyISAM还使用一种表格锁定的机制，来优化多个并发的读写操作，其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新机制所浪费的空间。</p><p>适用场景：<br>1）不支持事务的设计，但是并不代表着有事务操作的项目不能用MyISAM存储引擎，可以在service层进行根据自己的业务需求进行相应的控制。<br>2）不支持外键的表设计。<br>3）查询速度很快，如果数据库insert和update的操作比较多的话比较适用。<br>4）整天 对表进行加锁的场景。<br>5）MyISAM极度强调快速读取操作。<br>6）MyIASM中存储了表的行数，于是SELECT COUNT(*) FROM TABLE时只需要直接读取已经保存好的值而不需要进行全表扫描。如果表的读操作远远多于写操作且不需要数据库事务的支持，那么MyIASM也是很好的选择。<br>缺点：<br>就是不能在表损坏后恢复数据。（是不能主动恢复）</p><blockquote><p>补充：ISAM索引方法–索引顺序存取方法<br>定义：<br>是一个定义明确且历经时间考验的数据表格管理方法，它在设计之时就考虑到 数据库被查询的次&gt; 数要远大于更新的次数。<br>特性：<br>ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。<br>在设计之初就预想数据组织成有固定长度的记录，按顺序存储的。—ISAM是一种静态索引结构。<br>缺点：<br>1.它不 支持事务处理<br>2.也不能够容错。如果你的硬盘崩溃了，那么数据文件就无法恢复了。如果你正在把ISAM用在关键任务应用程序里，那就必须经常备份你所有的实 时数据，通过其复制特性，MYSQL能够支持这样的备份应用程序。</p></blockquote><h2 id="Memory（也叫HEAP）："><a href="#Memory（也叫HEAP）：" class="headerlink" title="Memory（也叫HEAP）："></a>Memory（也叫HEAP）：</h2><p>定义：<br>使用存在内存中的内容来创建表。每个MEMORY表只实际对应一个磁盘文件。MEMORY类型的表访问非常得快，因为它的数据是放在内存中的，并且默认使用HASH索引。<br>但是一旦服务关闭，表中的数据就会丢失掉。 HEAP允许只驻留在内存里的临时表格。驻留在内存里让HEAP要比ISAM和MYISAM都快，但是它所管理的数据是不稳定的，而且如果在关机之前没有进行保存，那么所有的数据都会丢失。在数据行被删除的时候，HEAP也不会浪费大量的空间。HEAP表格在你需要使用SELECT表达式来选择和操控数据的时候非常有用。</p><p>适用场景：<br>1）那些内容变化不频繁的代码表，或者作为统计操作的中间结果表，便于高效地堆中间结果进行分析并得到最终的统计结果。<br>2）目标数据比较小，而且非常频繁的进行访问，在内存中存放数据，如果太大的数据会造成内存溢出。可以通过参数max_heap_table_size控制Memory表的大小，限制Memory表的最大的大小。<br>3）数据是临时的，而且必须立即可用得到，那么就可以放在内存中。<br>4）存储在Memory表中的数据如果突然间丢失的话也没有太大的关系。<br>注意： Memory同时支持散列索引和B树索引，B树索引可以使用部分查询和通配查询，也可以使用&lt;,&gt;和&gt;=等操作符方便数据挖掘，散列索引相等的比较快但是对于范围的比较慢很多。<br>特性要求：<br>1）要求存储的数据是数据长度不变的格式，比如，Blob和Text类型的数据不可用（长度不固定的）。<br>2）要记住，在用完表格之后就删除表格。</p><h2 id="Archive"><a href="#Archive" class="headerlink" title="Archive"></a>Archive</h2><p>定义：<br>基本上用于数据归档；它的压缩比非常的高，存储空间大概是innodb的10-15分之一所以它用来存储历史数据非常的适合，由于它不支持索引同时也不能缓存索引和数据，所以它不适合作为并发访问表的存储引擎。Archivec存储引擎使用行锁来实现高并发插入操作，但是它不支持事务，其设计目标只是提供高速的插入和压缩功能。<br>Archiv存储引擎的特点：<br>1、往archive表插入的数据会使用zlib进行数据压缩，磁盘I/O更少，archive支持optimize table、 check table操作。<br>2、每个archive表在磁盘上存在两个文件 .frm(存储表定义) 和 .arz(存储数据)。</p><p>3、archive存储引擎支持insert、replace和select操作，但是不支持update和delete。</p><p>4、archive存储引擎支持blob、text等大字段类型。支持auto_increment自增列同时自增列可以不是唯一索引。</p><p>5、archive支持auto_increment列，但是不支持往auto_increment列插入一个小于当前最大的值的值。</p><p>6、archive不支持索引所以无法在archive表上创建主键、唯一索引、和一般的索引。</p><p>7、Archive表比MyISAM表要小大约75%，比支持事务处理的InnoDB表小大约83%。当数据量非常大的时候（达到1.5GB这个量级，CPU又比较快的时候）Archive的插入性能表现会较MyISAM为佳。</p><p>8、较小的空间占用，移植MySQL数据方便。当你需要把数据从一台MySQL服务器转移到另一台的时候，Archive表可以方便地移植到新的MySQL环境，你只需将保存Archive表的底层文件复制过去就可以了。</p><h2 id="Mrg-Myisam（水平分表）"><a href="#Mrg-Myisam（水平分表）" class="headerlink" title="Mrg_Myisam（水平分表）"></a>Mrg_Myisam（水平分表）</h2><p>定义：<br>是一个相同的可以被当作一个来用的MyISAM表的集合。“相同”意味着所有表同样的列和索引信息。<br>也就是说，他将MyIsam引擎的多个表聚合起来，但是它的内部没有数据，真正的数据依然是MyIsam引擎的表中，但是可以直接进行查询、删除更新等操作。<br>比如：我们可能会遇到这样的问题，同一种类的数据会根据数据的时间分为多个表，如果这时候进行查询的话，就会比较麻烦，Merge可以直接将多个表聚合成一个表统一查询，然后再删除Merge表（删除的是定义），原来的数据不会影响。</p><h2 id="Blackhole（黑洞引擎）"><a href="#Blackhole（黑洞引擎）" class="headerlink" title="Blackhole（黑洞引擎）"></a>Blackhole（黑洞引擎）</h2><p>定义<br>任何写入到此引擎的数据均会被丢弃掉， 不做实际存储；Select语句的内容永远是空。<br>他会丢弃所有的插入的数据，服务器会记录下Blackhole表的日志，所以可以用于复制数据到备份数据库。<br>使用场景：<br>1）验证dump file语法的正确性<br>2）以使用blackhole引擎来检测binlog功能所需要的额外负载<br>3）充当日志服务器</p><h2 id="存储引擎的选择"><a href="#存储引擎的选择" class="headerlink" title="存储引擎的选择"></a>存储引擎的选择</h2><p>不同的存储引擎都有各自的特点，以适应不同的需求，如下表所示：</p><table><thead><tr><th><strong>功  能</strong></th><th><strong>MyISAM</strong></th><th><strong>Memory</strong></th><th><strong>InnoDB</strong></th><th><strong>Archive</strong></th></tr></thead><tbody><tr><td>存储限制</td><td>256TB</td><td>RAM</td><td>64TB</td><td>None</td></tr><tr><td>支持事物</td><td>No</td><td>No</td><td>Yes</td><td>No</td></tr><tr><td>支持全文索引</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr><tr><td>支持数索引</td><td>Yes</td><td>Yes</td><td>Yes</td><td>No</td></tr><tr><td>支持哈希索引</td><td>No</td><td>Yes</td><td>No</td><td>No</td></tr><tr><td>支持数据缓存</td><td>No</td><td>N/A</td><td>Yes</td><td>No</td></tr><tr><td>支持外键</td><td>No</td><td>No</td><td>Yes</td><td>No</td></tr></tbody></table><p>如果要提供提交、回滚、崩溃恢复能力的事物安全（ACID兼容）能力，并要求实现并发控制，InnoDB是一个好的选择。</p><p>如果数据表主要用来插入和查询记录，则MyISAM引擎能提供较高的处理效率。</p><p>如果只是临时存放数据，数据量不大，并且不需要较高的数据安全性，可以选择将数据保存在内存中的Memory引擎，MySQL中使用该引擎作为临时表，存放查询的中间结果。</p><p>如果只有INSERT和SELECT操作，可以选择Archive，Archive支持高并发的插入操作，但是本身不是事务安全的。Archive非常适合存储归档数据，如记录日志信息可以使用Archive。</p><p>使用哪一种引擎需要灵活选择，<strong>一个数据库中多个表可以使用不同引擎以满足各种性能和实际需求</strong>，使用合适的存储引擎，将会提高整个数据库的性能。</p>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>浅谈线程中的 wait() 和 sleep()</title>
      <link href="/2017/12/17/%E6%B5%85%E8%B0%88%E7%BA%BF%E7%A8%8B%E4%B8%AD%E7%9A%84wait()%E5%92%8Csleep()/"/>
      <url>/2017/12/17/%E6%B5%85%E8%B0%88%E7%BA%BF%E7%A8%8B%E4%B8%AD%E7%9A%84wait()%E5%92%8Csleep()/</url>
      <content type="html"><![CDATA[<h1 id="（￣︶￣）"><a href="#（￣︶￣）" class="headerlink" title="\（￣︶￣）/"></a>\（￣︶￣）/</h1><p>&nbsp; &nbsp; &nbsp; &nbsp; wait() 方法和 sleep() 方法都可以使线程挂起，起到的效果看似相同，但其实二者之间存在许多差异。 </p><h3 id="相同点："><a href="#相同点：" class="headerlink" title="相同点："></a>相同点：</h3><p>(1) 都可以使线程在程序的调用处阻塞指定的毫秒数，然后回到可运行状态。</p><p>(2) wait() 和 sleep() 都可以通过 interrupt() 方法打断线程的暂停状态（不建议使用该方法）。</p><blockquote><p>注：对线程对象调用 interrupt() 方法时，如果该线程对象处于 wait / sleep / join 状态时，该线程会立刻抛出 InterruptedException，在catch() {} 中直接 return 即可安全地结束线程；如果该线程正在执行的是普通代码，那么该线程不会抛出 InterruptedException。<br><a id="more"></a></p></blockquote><h3 id="不同点："><a href="#不同点：" class="headerlink" title="不同点："></a>不同点：</h3><p>(1) sleep() 是Thread类的static(静态)的方法。（ yield() 同样为静态方法）。<br>   wait() 是非静态方法，需要由线程对象来调用。（ 非静态方法还有 join(), notify(), notifyAll()等 )。</p><p>(2) sleep()睡眠时，不会释放对象锁，有造成死锁的可能；<br>   wait() 等待时，释放对象锁，其他线程可以访问。</p><p>(3) wait()、notify() 和 notifyAll() 只能在同步控制方法或者同步控制块里面使用，而 sleep() 可以在任何地方使用。</p><p>(4) sleep() 必须捕获异常，而wait()，notify() 和 notifyAll() 不需要捕获异常</p><blockquote><p>在 sleep() 休眠时间期满后，该线程不一定会立即执行，这是因为其它线程可能正在运行而且没有被调度为放弃执行，除非此线程具有更高的优先级。<br> wait() 使用 notify() 或者 notifyAll() 或者指定睡眠时间来唤醒当前等待池中的线程。wiat() 必须放在synchronized block中，否则会在program runtime时抛出“java.lang.IllegalMonitorStateException”异常。</p></blockquote><h3 id="线程的优先级"><a href="#线程的优先级" class="headerlink" title="线程的优先级"></a>线程的优先级</h3><p>优先级越高代表被cpu执行的概率越大（1到10），一般1、5与10的差距是最大的。 因为是静态常量，因此需要加上类名 。</p><blockquote><p>字段摘要:Thread.MAX_PRIORITY，线程可以具有的最高优先级；Thread.MIN_PRIORITY，线程可以具有的最低优先级；Thread.NORM_PRIORITY，分配给线程的默认优先级，例如：</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">implements</span> <span class="title">Runnable</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> x=<span class="number">0</span>;x&lt;<span class="number">50</span>;x++)&#123;</span><br><span class="line">        System.out.println(Thread.currentThread().getName()+<span class="string">"......"</span>+x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JoinDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        MyThread mt = <span class="keyword">new</span> MyThread();</span><br><span class="line">        Thread t1 = <span class="keyword">new</span> Thread(mt);</span><br><span class="line">        Thread t2 = <span class="keyword">new</span> Thread(mt);</span><br><span class="line">        t1.start();</span><br><span class="line">        <span class="comment">//临时加入一个线程运算时可以使用join方法</span></span><br><span class="line">        t1.join();<span class="comment">//t1线程要申请加入进来运行，主线程等待t1终止之后再执行,即此时主线程处于冻结状态，是可以使用interrupt方法强制恢复回来的</span></span><br><span class="line">        t2.start();</span><br><span class="line">        t2.setPriority(Thread.MAX_PRIORITY);<span class="comment">//设置t2为最优先执行</span></span><br><span class="line">        <span class="comment">//t1.join();//t1线程申请加入；此时开启的线程是t1与t2，即t1与t2互相争夺执行权，但是主线程仅仅与t1线程相关联，在线程t1执行结束后主线程开始执行</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> x=<span class="number">0</span>;x&lt;<span class="number">50</span>;x++)&#123;</span><br><span class="line">            System.out.println(Thread.currentThread().getName()+<span class="string">"......"</span>+x);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Thread-yield-方法"><a href="#Thread-yield-方法" class="headerlink" title="Thread.yield() 方法"></a>Thread.yield() 方法</h3><p>yield() 方法：暂停当前正在执行的线程对象，并执行其他线程。<br>yield() 应该做的是让当前运行线程回到可运行状态，以允许具有相同优先级的其他线程获得运行机会。<br>因此，使用 yield() 的目的是让相同优先级的线程之间能适当的轮转执行。但是，实际中无法保证 yield() 达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。</p>]]></content>
      
      <categories>
          
          <category> Core Java基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Thread </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Dubbo的集群容错和负载均衡</title>
      <link href="/2017/10/21/Dubbo%E7%9A%84%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
      <url>/2017/10/21/Dubbo%E7%9A%84%E9%9B%86%E7%BE%A4%E5%AE%B9%E9%94%99%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
      <content type="html"><![CDATA[<h1 id="o-O"><a href="#o-O" class="headerlink" title="o_O"></a>o_O</h1><h2 id="容错机制"><a href="#容错机制" class="headerlink" title="容错机制"></a>容错机制</h2><p>&nbsp; &nbsp; &nbsp; &nbsp; Dubbo中常见容错机制包括 failover，failsafe，failfase，failback，forking，缺省为failover重试。</p><p><font color="#1A75B1"><strong>Failover  &nbsp;失败自动切换</strong></font><br>当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过retries=”2”来设置重试次数(不含第一次)。</p><p><font color="#1A75B1"><strong>Failfast  &nbsp;快速失败</strong></font><br>只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。</p><p><font color="#1A75B1"><strong>Failsafe  &nbsp;失败安全</strong></font><br>出现异常时，直接忽略，通常用于写入审计日志等操作，调用信息丢失。可用于生产环境 Monitor。<br><a id="more"></a></p><p><font color="#1A75B1"><strong>Failback  &nbsp;失败自动恢复</strong></font><br>后台记录失败请求，定时重发，通常用于消息通知操作。可用于生产环境 Registry。</p><p><font color="#1A75B1"><strong>Forking  &nbsp;并行调用多个服务器</strong></font><br>并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过forks=”2”来设置最大并行数。</p><p><font color="#1A75B1"><strong>Broadcast  &nbsp;广播调用</strong></font><br>广播调用所有提供者，逐个调用，任意一台报错则报错。(2.1.0开始支持) 通常用于通知所有提供者更新缓存或日志等本地资源信息。</p><h2 id="负载均衡策略"><a href="#负载均衡策略" class="headerlink" title="负载均衡策略"></a>负载均衡策略</h2><p>&nbsp; &nbsp; &nbsp; &nbsp; Dubbo提供了多种均衡策略，缺省为random随机调用。</p><p><font color="#1A75B1"><strong>Random LoadBalance  &nbsp;随机</strong></font><br>按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。</p><p><font color="#1A75B1"><strong>RoundRobin LoadBalance  &nbsp;轮循</strong></font><br>按公约后的权重设置轮循比率，存在慢的提供者累积请求问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。</p><p><font color="#1A75B1"><strong>LeastActive LoadBalance  &nbsp;最少活跃调用数</strong></font><br>相同活跃数的随机。活跃数指调用前后计数差，使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。</p><p><font color="#1A75B1"><strong>ConsistentHash LoadBalance  &nbsp; 一致性Hash</strong></font><br>相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。</p>]]></content>
      
      <categories>
          
          <category> 集群容错 </category>
          
          <category> 负载均衡 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dubbo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>数据库的事务隔离级别</title>
      <link href="/2017/10/19/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"/>
      <url>/2017/10/19/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/</url>
      <content type="html"><![CDATA[<h1 id="￣ε￣；"><a href="#￣ε￣；" class="headerlink" title="(￣ε￣；)"></a>(￣ε￣；)</h1><h2 id="事务的四个特性"><a href="#事务的四个特性" class="headerlink" title="事务的四个特性"></a>事务的四个特性</h2><p>数据库事务（Transaction）是指作为单个逻辑工作单元执行的一系列操作，要么完全地执行，要么完全地不执行。一方面，当多个应用程序并发访问数据库时，事务可以在应用程序间提供一个隔离方法，防止互相干扰。另一方面，事务为数据库操作序列提供了一个从失败恢复正常的方法。</p><p>事务具有四个特性：原子性（Atomicity）、一致性（Consistency）、隔离型（Isolation）、持久性（Durability），简称ACID。</p><p>1 原子性（Atomicity）<br>事务的原子性是指事务中的操作不可拆分，只允许全部执行或者全部不执行。<br><a id="more"></a><br>2 一致性（Consistency）<br>事务的一致性是指事务的执行不能破坏数据库的一致性，一致性也称为完整性。一个事务在执行后，数据库必须从一个一致性状态转变为另一个一致性状态。</p><p>3 隔离型（Isolation）<br>事务的隔离型是指并发的事务相互隔离，不能互相干扰。</p><p>4 持久性（Durability）<br>事务的持久性是指事务一旦提交，对数据的状态变更应该被永久保存。</p><h2 id="数据库事务隔离级别"><a href="#数据库事务隔离级别" class="headerlink" title="数据库事务隔离级别"></a>数据库事务隔离级别</h2><p>数据库事务的隔离级别有4个，由低到高依次为Read uncommitted 、Read committed 、Repeatable read 、Serializable ，这三个级别（从Read committed起）可以逐个解决脏读 、不可重复读 、幻读这几类问题。</p><blockquote><p>&nbsp; √: 可能出现        &nbsp; &nbsp; &nbsp; &nbsp; ×: 不会出现</p></blockquote><table><tr><td>级别</td><td>脏读</td><td>不可重复读</td><td>幻读</td></tr><tr><td>Read uncommitted</td><td> √ </td><td> √ </td><td> √ </td></tr><tr><td>Read committed</td><td> × </td><td> √ </td><td> √ </td></tr><tr><td>Repeatable read</td><td> × </td><td> × </td><td> √ </td></tr><tr><td>Serializable</td><td> × </td><td> × </td><td> × </td></tr> </table><blockquote><p>注意：我们讨论隔离级别的场景，主要是在多个事务并发 的情况下，因此，接下来的讲解都围绕事务并发。</p></blockquote><h3 id="Read-Uncommitted-读未提交"><a href="#Read-Uncommitted-读未提交" class="headerlink" title="Read Uncommitted 读未提交"></a>Read Uncommitted 读未提交</h3><p>公司发工资了，领导把5000元打到singo的账号上，但是该事务并未提交，而singo正好去查看账户，发现工资已经到账，是5000元整，非常高兴。可是不幸的是，领导发现发给singo的工资金额不对，是2000元，于是迅速回滚了事务，修改金额后，将事务提交，最后singo实际的工资只有 2000元，singo空欢喜一场。</p><p>出现上述情况，即我们所说的脏读 ，两个并发的事务，“事务A：领导给singo发工资”、“事务B：singo查询工资账户”，事务B读取了事务A尚未提交的数据。</p><p>当隔离级别设置为Read uncommitted 时，就可能出现脏读。</p><h3 id="Read-Committed-读提交"><a href="#Read-Committed-读提交" class="headerlink" title="Read Committed 读提交"></a>Read Committed 读提交</h3><p>singo拿着工资卡去消费，系统读取到卡里确实有2000元，而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到另一账户，并在singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为何……</p><p>出现上述情况，即我们所说的不可重复读 ，两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”，事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。</p><p>当隔离级别设置为Read committed 时，避免了脏读，但是可能会造成不可重复读。</p><blockquote><p>大多数数据库的默认级别就是Read Committed，比如Sql Server , Oracle。</p></blockquote><h3 id="Repeatable-Read-重复读"><a href="#Repeatable-Read-重复读" class="headerlink" title="Repeatable Read 重复读"></a>Repeatable Read 重复读</h3><p>当隔离级别设置为Repeatable read 时，可以避免不可重复读。当singo拿着工资卡去消费时，一旦系统开始读取工资卡信息（即事务开始），singo的老婆就不可能对该记录进行修改，也就是singo的老婆不能在此时转账。</p><p>虽然Repeatable read避免了不可重复读，但还有可能出现幻读 。<strong>不可重复读对应的是修改</strong>，即<font color="red">UPDATE</font>操作。但是可能还会有幻读问题。因为<strong>幻读问题对应的是插入<font color="red">INSERT</font></strong>操作，而不是UPDATE操作。</p><p>singo的老婆工作在银行部门，她时常通过银行内部系统查看singo的信用卡消费记录。有一天，她正在查询到singo当月信用卡的总消费金额（select sum(amount) from transaction where month = 本月）为80元，而singo此时正好在外面胡吃海塞后在收银台买单，消费1000元，即<font color="red">新增</font>了一条1000元的消费记录（<font color="red">insert</font> transaction … ），并提交了事务，随后singo的老婆将singo当月信用卡消费的明细打印到A4纸上，却发现消费总额为1080元，singo的老婆很诧异，以为出现了幻觉，幻读就这样产生了。</p><blockquote><p>注：Mysql的默认隔离级别就是Repeatable Read。</p></blockquote><h3 id="Serializable-序列化"><a href="#Serializable-序列化" class="headerlink" title="Serializable 序列化"></a>Serializable 序列化</h3><p>Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。</p><h2 id="设置事务的指令"><a href="#设置事务的指令" class="headerlink" title="设置事务的指令"></a>设置事务的指令</h2><p>设置事务级别：SET TRANSACTION ISOLATION LEVEL</p><p>开始事务：begin tran<br>mysql : begin; | begin work; | start transaction; </p><p>提交事务：COMMIT</p><p>回滚事务：ROLLBACK</p><p>创建事务保存点：SAVE TRANSACTION savepoint_name</p><p>回滚到事务点:ROLLBACK TRANSACTION savepoint_name</p>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database </tag>
            
            <tag> Transaction </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>悲观锁和乐观锁</title>
      <link href="/2017/10/06/%E6%82%B2%E8%A7%82%E9%94%81%E5%92%8C%E4%B9%90%E8%A7%82%E9%94%81/"/>
      <url>/2017/10/06/%E6%82%B2%E8%A7%82%E9%94%81%E5%92%8C%E4%B9%90%E8%A7%82%E9%94%81/</url>
      <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title="@_@"></a>@_@</h1><p>&nbsp; &nbsp; &nbsp; &nbsp;悲观锁还是乐观锁是人们定义的一种概念，可以认为是一种思想。其实不仅仅是关系型数据库系统中有乐观锁和悲观锁的概念，像memcached、hibernate、tair等都有类似的概念。<br>&nbsp; &nbsp; &nbsp; &nbsp;针对于不同的业务场景，应该选用不同的并发控制方式。所以，不要把乐观并发控制和悲观并发控制狭义的理解为DBMS中的概念，更不要把他们和数据中提供的锁机制（行锁、表锁、排他锁、共享锁）混为一谈。其实，在DBMS中，悲观锁正是利用数据库本身提供的锁机制来实现的。<br>&nbsp;&nbsp;&nbsp;&nbsp;下面具体介绍一下悲观锁和乐观锁。<br><a id="more"></a></p><h2 id="悲观锁"><a href="#悲观锁" class="headerlink" title="悲观锁"></a>悲观锁</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;在关系数据库管理系统里，悲观并发控制（又名“悲观锁”，Pessimistic Concurrency Control，缩写“PCC”）是一种并发控制的方法。它默认对每次数据操作加锁。如果一个事务执行的操作对某行数据应用了锁，那只有当这个事务把锁释放，其他事务才能够执行与该锁冲突的操作。悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。<br>&nbsp; &nbsp; &nbsp; &nbsp;悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。</p></blockquote><h3 id="在数据库中，悲观锁的流程如下："><a href="#在数据库中，悲观锁的流程如下：" class="headerlink" title="在数据库中，悲观锁的流程如下："></a>在数据库中，悲观锁的流程如下：</h3><p>1) 在对任意记录进行修改前，先尝试为该记录加上排他锁（Exclusive Lock）。<br>2) 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。 具体响应方式由开发者根据实际需要决定。<br>3) 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。<br>4) 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。</p><h3 id="MySQL-InnoDB-中使用悲观锁"><a href="#MySQL-InnoDB-中使用悲观锁" class="headerlink" title="MySQL(InnoDB)中使用悲观锁"></a>MySQL(InnoDB)中使用悲观锁</h3><blockquote><p>要使用悲观锁，我们必须关闭mysql数据库的自动提交属性，因为MySQL默认使用autocommit模式，也就是说，当你执行一个更新操作后，MySQL会立刻将结果进行提交。set autocommit=0;</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//0.开始事务</span><br><span class="line"><span class="keyword">begin</span>;/<span class="keyword">begin</span> <span class="keyword">work</span>;/<span class="keyword">start</span> <span class="keyword">transaction</span>; (三者选一就可以)</span><br><span class="line">//1.查询出商品信息</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">status</span> <span class="keyword">from</span> t_goods <span class="keyword">where</span> <span class="keyword">id</span>=<span class="number">1</span> <span class="keyword">for</span> <span class="keyword">update</span>;</span><br><span class="line">//2.根据商品信息生成订单</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> t_orders (<span class="keyword">id</span>,goods_id) <span class="keyword">values</span> (<span class="literal">null</span>,<span class="number">1</span>);</span><br><span class="line">//3.修改商品status为2</span><br><span class="line"><span class="keyword">update</span> t_goods <span class="keyword">set</span> <span class="keyword">status</span>=<span class="number">2</span>;</span><br><span class="line">//4.提交事务</span><br><span class="line"><span class="keyword">commit</span>;/<span class="keyword">commit</span> <span class="keyword">work</span>;</span><br></pre></td></tr></table></figure><p>&nbsp; &nbsp; &nbsp; &nbsp;上面的查询语句中，我们使用了select…for update的方式，这样就通过开启排他锁的方式实现了悲观锁。此时在t_goods表中，id为1的 那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。</p><blockquote><p>使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁把整张表锁住，这点需要注意。</p></blockquote><h3 id="悲观锁的优点与不足"><a href="#悲观锁的优点与不足" class="headerlink" title="悲观锁的优点与不足"></a>悲观锁的优点与不足</h3><p>&nbsp; &nbsp; &nbsp; &nbsp;悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加了产生死锁的机率；另外，在只读型事务处理中由于不会产生冲突，没必要使用锁，这样做只能增加系统负载；再者会降低程序并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以继续操作。</p><h2 id="乐观锁"><a href="#乐观锁" class="headerlink" title="乐观锁"></a>乐观锁</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;在关系数据库管理系统里，乐观并发控制（又名“乐观锁”，Optimistic Concurrency Control，缩写“OCC”）是一种并发控制的方法。它假设多用户并发的事务在处理时不会彼此相互影响，各事务能够在不产生锁的情况下处理各自影响的那部分数据。在提交数据更新之前，每个事务会先检查在该事务读取数据后，有没有其他事务又修改了该数据。如果其他事务有更新的话，正在提交的事务会进行回滚。乐观事务控制最早是由孔祥重（H.T.Kung）教授提出。</p></blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;相对于悲观锁而言，乐观锁（Optimistic Lock）假定认为数据操作一般情况下不会发生冲突，所以在提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则让返回用户错误的信息，让用户决定如何去做。java.util.concurrent 包中 CompareAndSet (CAS，失败-重试) 就是一种乐观锁机制。<br>相对于悲观锁，在对数据库进行处理的时候，乐观锁并不会使用数据库提供的锁机制。一般的实现&nbsp; &nbsp; &nbsp; &nbsp;乐观锁的方式就是记录数据版本，或者使用时间戳。</p><blockquote><p>记录数据版本，即为为数据增加的一个版本标识。当读取数据时，将版本标识的值一同读出，数据每更新一次，同时对版本标识进行更新。当我们提交更新的时候，会先对比数据库表对应记录的当前版本信息与第一次取出来的版本标识，如果数据库表当前版本号与第一次取出来的版本标识值相等，则予以更新，否则认为是过期数据。</p></blockquote><h3 id="使用版本号实现乐观锁"><a href="#使用版本号实现乐观锁" class="headerlink" title="使用版本号实现乐观锁"></a>使用版本号实现乐观锁</h3><p>&nbsp; &nbsp; &nbsp; &nbsp;使用版本号时，可以在数据初始化时指定一个版本号，每次对数据的更新操作都对版本号执行+1操作。并判断当前版本号是不是该数据的最新的版本号。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1.查询出商品信息</span><br><span class="line"><span class="keyword">select</span> (<span class="keyword">status</span>,<span class="keyword">status</span>,<span class="keyword">version</span>) <span class="keyword">from</span> t_goods <span class="keyword">where</span> <span class="keyword">id</span>=#&#123;<span class="keyword">id</span>&#125;</span><br><span class="line"><span class="number">2.</span>根据商品信息生成订单</span><br><span class="line"><span class="number">3.</span>修改商品<span class="keyword">status</span>为<span class="number">2</span></span><br><span class="line"><span class="keyword">update</span> t_goods </span><br><span class="line"><span class="keyword">set</span> <span class="keyword">status</span>=<span class="number">2</span>,<span class="keyword">version</span>=<span class="keyword">version</span>+<span class="number">1</span></span><br><span class="line"><span class="keyword">where</span> <span class="keyword">id</span>=#&#123;<span class="keyword">id</span>&#125; <span class="keyword">and</span> <span class="keyword">version</span>=#&#123;<span class="keyword">version</span>&#125;;</span><br></pre></td></tr></table></figure></p><h3 id="乐观锁的优点与不足"><a href="#乐观锁的优点与不足" class="headerlink" title="乐观锁的优点与不足"></a>乐观锁的优点与不足</h3><p>&nbsp; &nbsp; &nbsp; &nbsp;乐观并发控制相信事务之间的数据竞争(Data Race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。</p>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database </tag>
            
            <tag> Lock </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>数据库的锁机制</title>
      <link href="/2017/10/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6/"/>
      <url>/2017/10/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6/</url>
      <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title="(=^_^=)"></a>(=^_^=)</h1><p>&nbsp; &nbsp; &nbsp; &nbsp;在计算机科学中，特别是程序设计、操作系统、多处理机和数据库等领域，并发控制（Concurrency Control）是确保及时纠正由并发操作导致的错误的一种机制。<br>&nbsp; &nbsp; &nbsp; &nbsp;数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。下面举例说明并发操作带来的数据不一致性问题：<br><a id="more"></a></p><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;现有两处火车票售票点，同时读取某一趟列车车票数据库中车票余额为 X。两处售票点同时卖出一张车票，同时修改余额为 X - 1写回数据库，这样就造成了实际卖出两张火车票而数据库中的记录却只少了一张。 产生这种情况的原因是因为两个事务读入同一数据并同时修改，其中一个事务提交的结果破坏了另一个事务提交的结果，导致其数据的修改被丢失，破坏了事务的隔离性。并发控制要解决的就是这类问题。</p></blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;封锁、时间戳、乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。</p><h2 id="加锁"><a href="#加锁" class="headerlink" title="加锁"></a>加锁</h2><p>&nbsp; &nbsp; &nbsp; &nbsp;当并发事务同时访问一个资源时，有可能导致数据不一致，因此需要一种机制来将数据访问顺序化，以保证数据库数据的一致性。锁就是其中的一种机制。在计算机科学中，锁是在执行多线程时用于强行限制资源访问的同步机制，即用于在并发控制中保证对互斥要求的满足。</p><h2 id="锁的分类-oracle"><a href="#锁的分类-oracle" class="headerlink" title="锁的分类(oracle)"></a>锁的分类(oracle)</h2><p>1）按操作划分，可分为DML锁、DDL锁<br>2）按锁的粒度划分，可分为表级锁、行级锁、页级锁（mysql）<br>3）按锁级别划分，可分为共享锁、排他锁<br>4）按加锁方式划分，可分为自动锁、显示锁<br>5）按使用方式划分，可分为乐观锁、悲观锁</p><blockquote><p>DML锁（又称 Data Lock，数据锁），用于保护数据的完整性，其中包括行级锁(Row Lock (TX锁))、表级锁(Table Lock (TM锁))。<br>DDL锁（又称 Dictionary Lock，数据字典锁），用于保护数据库对象的结构，如表、索引等的结构定义。包括排他DDL锁（Exclusive DDL Lock）、共享DDL锁（Share DDL Lock）、可中断解析锁（Breakable Parse Lock）。</p></blockquote>]]></content>
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database </tag>
            
            <tag> Lock </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>JVM的类加载机制</title>
      <link href="/2017/09/21/JVM%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/"/>
      <url>/2017/09/21/JVM%E7%9A%84%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</url>
      <content type="html"><![CDATA[<h1 id="（￣Q￣）╯"><a href="#（￣Q￣）╯" class="headerlink" title="（￣Q￣）╯"></a>（￣Q￣）╯</h1><h2 id="类加载过程"><a href="#类加载过程" class="headerlink" title="类加载过程"></a>类加载过程</h2><p>如下图所示，JVM类加载机制分为五个部分：加载，验证，准备，解析，初始化，下面我们就分别来看一下这五个过程。<br><img src="/images/jvm/loadclass.png" alt="Alt text"><br><a id="more"></a><br><strong>加载</strong><br>加载是类加载过程中的一个阶段，这个阶段会在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的入口。注意这里不一定非得要从一个Class文件获取，这里既可以从ZIP包中读取（比如从jar包和war包中读取），也可以在运行时计算生成（动态代理），也可以由其它文件生成（比如将JSP文件转换成对应的Class类）。</p><p><strong>验证</strong><br>这一阶段的主要目的是为了确保Class文件的字节流中包含的信息是否符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。</p><p><strong>准备</strong><br>准备阶段是正式为类变量分配内存并设置类变量的初始值阶段，即在方法区中分配这些变量所使用的内存空间。注意这里所说的初始值概念，比如一个类变量定义为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> v = <span class="number">8080</span>;</span><br></pre></td></tr></table></figure></p><p>实际上变量v在准备阶段过后的初始值为0而不是8080，将v赋值为8080的putstatic指令是程序被编译后，存放于类构造器<client>方法之中，这里我们后面会解释。<br>但是注意如果声明为：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> v = <span class="number">8080</span>;</span><br></pre></td></tr></table></figure></client></p><p>在编译阶段会为v生成ConstantValue属性，在准备阶段虚拟机会根据ConstantValue属性将v赋值为8080。</p><p><strong>解析</strong><br>解析阶段是指虚拟机将常量池中的符号引用替换为直接引用的过程。符号引用就是class文件中的：</p><p>CONSTANT_Class_info<br>CONSTANT_Field_info<br>CONSTANT_Method_info<br>等类型的常量。</p><p>下面我们解释一下符号引用和直接引用的概念：</p><p>符号引用与虚拟机实现的布局无关，引用的目标并不一定要已经加载到内存中。各种虚拟机实现的内存布局可以各不相同，但是它们能接受的符号引用必须是一致的，因为符号引用的字面量形式明确定义在Java虚拟机规范的Class文件格式中。<br>直接引用可以是指向目标的指针，相对偏移量或是一个能间接定位到目标的句柄。如果有了直接引用，那引用的目标必定已经在内存中存在。</p><p><strong>初始化</strong><br>初始化阶段是类加载最后一个阶段，前面的类加载阶段之后，除了在加载阶段可以自定义类加载器以外，其它操作都由JVM主导。到了初始阶段，才开始真正执行类中定义的Java程序代码。</p><p>初始化阶段是执行类构造器<client>方法的过程。<client>方法是由编译器自动收集类中的类变量的赋值操作和静态语句块中的语句合并而成的。虚拟机会保证<client>方法执行之前，父类的<client>方法已经执行完毕。p.s: 如果一个类中没有对静态变量赋值也没有静态语句块，那么编译器可以不为这个类生成<client>()方法。</client></client></client></client></client></p><p>注意以下几种情况不会执行类初始化：</p><p>(1) 通过子类引用父类的静态字段，只会触发父类的初始化，而不会触发子类的初始化。<br>(2) 定义对象数组，不会触发该类的初始化。<br>(3) 常量在编译期间会存入调用类的常量池中，本质上并没有直接引用定义常量的类，不会触发定义常量所在的类。<br>(4) 通过类名获取Class对象，不会触发类的初始化。<br>(5) 通过Class.forName加载指定类时，如果指定参数initialize为false时，也不会触发类初始化，其实这个参数是告诉虚拟机，是否要对类进行初始化。<br>(6) 通过ClassLoader默认的loadClass方法，也不会触发初始化动作。</p><h2 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h2><p>虚拟机设计团队把加载动作放到JVM外部实现，以便让应用程序决定如何获取所需的类，JVM提供了3种类加载器：<br><img src="/images/jvm/loader.png" alt="Alt text"><br><em>启动类加载器</em>(Bootstrap ClassLoader)：负责加载 JAVA_HOME\lib 目录中的，或通过-Xbootclasspath参数指定路径中的，且被虚拟机认可（按文件名识别，如rt.jar）的类。<br>扩展类加载器(Extension ClassLoader)：负责加载 JAVA_HOME\lib\ext 目录中的，或通过java.ext.dirs系统变量指定路径中的类库。<br><em>应用程序类加载器</em>(Application ClassLoader)：负责加载用户路径（classpath）上的类库。<br>JVM通过<strong>双亲委派模型</strong>进行类的加载，当然我们也可以通过继承java.lang.ClassLoader实现自定义的类加载器。</p><blockquote><p>双亲委派模型：当一个类加载器收到类加载任务，会先交给其父类加载器去完成，因此最终加载任务都会传递到顶层的启动类加载器，只有当父类加载器无法完成加载任务时，才会尝试执行加载任务。</p></blockquote><p>采用双亲委派的一个好处是比如加载位于rt.jar包中的类java.lang.Object，不管是哪个加载器加载这个类，最终都是委托给顶层的启动类加载器进行加载，这样就保证了使用不同的类加载器最终得到的都是同样一个Object对象。</p><p>在有些情境中可能会出现要我们自己来实现一个类加载器的需求，由于这里涉及的内容比较广泛，我想以后单独写一篇文章来讲述，不过这里我们还是稍微来看一下。我们直接看一下jdk中的ClassLoader的源码实现：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">synchronized</span> Class&lt;?&gt; loadClass(String name, <span class="keyword">boolean</span> resolve)</span><br><span class="line">        <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="comment">// First, check if the class has already been loaded</span></span><br><span class="line">    Class c = findLoadedClass(name);</span><br><span class="line">    <span class="keyword">if</span> (c == <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (parent != <span class="keyword">null</span>) &#123;</span><br><span class="line">                c = parent.loadClass(name, <span class="keyword">false</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                c = findBootstrapClass0(name);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            <span class="comment">// If still not found, then invoke findClass in order</span></span><br><span class="line">            <span class="comment">// to find the class.</span></span><br><span class="line">            c = findClass(name);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (resolve) &#123;</span><br><span class="line">        resolveClass(c);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>首先通过Class c = findLoadedClass(name);判断一个类是否已经被加载过。<br>如果没有被加载过执行if (c == null)中的程序，遵循双亲委派的模型，首先会通过递归从父加载器开始找，直到父类加载器是Bootstrap ClassLoader为止。<br>最后根据resolve的值，判断这个class是否需要解析。<br>而上面的findClass()的实现如下，直接抛出一个异常，并且方法是protected，很明显这是留给我们开发者自己去实现的。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> Class&lt;?&gt; findClass(String name) <span class="keyword">throws</span> ClassNotFoundException &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ClassNotFoundException(name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> Java 虚拟机 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>JVM中的垃圾收集算法及回收器</title>
      <link href="/2017/09/16/JVM%E4%B8%AD%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95%E5%8F%8A%E5%9B%9E%E6%94%B6%E5%99%A8/"/>
      <url>/2017/09/16/JVM%E4%B8%AD%E7%9A%84%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AE%97%E6%B3%95%E5%8F%8A%E5%9B%9E%E6%94%B6%E5%99%A8/</url>
      <content type="html"><![CDATA[<h1 id="（-￣▽￣-）"><a href="#（-￣▽￣-）" class="headerlink" title="（#￣▽￣#）"></a>（#￣▽￣#）</h1><h2 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h2><h3 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记 - 清除算法"></a>标记 - 清除算法</h3><p>标记-清除算法将垃圾回收分为两个阶段：标记阶段和清除阶段。在标记阶段首先通过根节点，标记所有从根节点开始的对象，未被标记的对象就是未被引用的垃圾对象。然后，在清除阶段，清除所有未被标记的对象。<br>标记清除算法主要不足有两个：一是效率问题，标记和清除两个过程效率都不高；另一个是标记清除之后会产生大量的空间碎片，因为回收后的空间是不连续的，这样给大对象分配内存的时候可能会提前触发full gc。<br><a id="more"></a></p><h3 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h3><p>将现有的内存空间分为两快，每次只使用其中一块，在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中，之后，清除正在使用的内存块中的所有对象，交换两个内存的角色，完成垃圾回收。</p><p>现在的商业虚拟机都采用这种收集算法来回收新生代，IBM研究表明新生代中的对象98%是朝夕生死的，所以并不需要按照1:1的比例划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中的一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地拷贝到另外一个Survivor空间上，最后清理掉Eden和刚才用过的Survivor的空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8:1(可以通过-SurvivorRattio来配置)，也就是每次新生代中可用内存空间为整个新生代容量的90%，只有10%的内存会被“浪费”。当然，98%的对象可回收只是一般场景下的数据，我们没有办法保证回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保。</p><h3 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记 - 整理算法"></a>标记 - 整理算法</h3><p>复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活的对象较多，复制的成本也将很高。<br>标记 - 整理算法是一种老年代的回收算法，它在标记-清除算法的基础上做了一些优化。首先也需要从根节点开始对所有可达对象做一次标记，但之后，它并不简单地清理未标记的对象，而是将所有的存活对象_压缩_到内存的一端。之后，清理边界外所有的空间。这种方法既避免了碎片的产生，又不需要两块相同的内存空间。</p><h3 id="增量算法"><a href="#增量算法" class="headerlink" title="增量算法"></a>增量算法</h3><p>增量算法的基本思想是，如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程和应用程序线程交替执行。每次，垃圾收集线程只收集一小片区域的内存空间，接着切换到应用程序线程。依次反复，直到垃圾收集完成。使用这种方式，由于在垃圾回收过程中，间断性地还执行了应用程序代码，所以能减少系统的停顿时间。但是，因为线程切换和上下文转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。</p><blockquote><p>分代收集算法<br>当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，新生代采用复制算法，老年代采用标记 - 整理算法。</p></blockquote><h2 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a>垃圾回收器</h2><h3 id="Serial收集器"><a href="#Serial收集器" class="headerlink" title="Serial收集器"></a>Serial收集器</h3><p>Serial收集器是最古老的收集器，它的缺点是当Serial收集器想进行垃圾回收的时候，必须暂停用户的所有进程，即stop the world。到现在为止，它依然是虚拟机运行在client模式下的默认新生代收集器，与其他收集器相比，对于限定在单个CPU的运行环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾回收自然可以获得最高的单线程收集效率。</p><p><strong>Serial Old</strong>是Serial收集器的老年代版本，它同样是一个单线程收集器，使用”标记－整理“算法。这个收集器的主要意义也是被Client模式下的虚拟机使用。在Server模式下，它主要还有两大用途：一个是在JDK1.5及以前的版本中与Parallel Scanvenge收集器搭配使用，另外一个就是作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure的时候使用。</p><p>通过指定-UseSerialGC参数，使用Serial + Serial Old的串行收集器组合进行内存回收。</p><h3 id="ParNew收集器"><a href="#ParNew收集器" class="headerlink" title="ParNew收集器"></a>ParNew收集器</h3><p>ParNew收集器是Serial收集器新生代的多线程实现，注意在进行垃圾回收的时候依然会stop the world，只是相比较Serial收集器而言它会运行多条线程进行垃圾回收。</p><p>ParNew收集器在不多于两个CPU的环境中并不会有太好的收集效果（相较于Serial收集器），但随着可以使用的CPU的数量增加，它对于GC时系统资源的利用还是很有好处的。它默认开启的收集线程数与CPU的数量相同，在CPU非常多（譬如32个，现在CPU动辄4核加超线程，服务器超过32个逻辑CPU的情况越来越多了）的环境下，可以使用-XX:ParallelGCThreads参数来限制垃圾收集的线程数。</p><p>-UseParNewGC: 打开此开关后，使用ParNew + Serial Old的收集器组合进行内存回收，这样新生代使用并行收集器，老年代使用串行收集器。</p><h3 id="Parallel-Scavenge收集器"><a href="#Parallel-Scavenge收集器" class="headerlink" title="Parallel Scavenge收集器"></a>Parallel Scavenge收集器</h3><p>Parallel是采用复制算法的多线程新生代垃圾回收器，似乎和ParNew收集器有很多的相似的地方。但是Parallel Scanvenge收集器的一个特点是它所关注的目标是吞吐量(Throughput)。所谓吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量=运行用户代码时间 / (运行用户代码时间 + 垃圾收集时间)。停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能够提升用户的体验；而高吞吐量则可以最高效率地利用CPU时间，尽快地完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。</p><p>Parallel Old收集器是Parallel Scavenge收集器的老年代版本，采用多线程和”标记－整理”算法。这个收集器是在jdk1.6中才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于比较尴尬的状态。原因是如果新生代Parallel Scavenge收集器，那么老年代除了Serial Old(PS MarkSweep)收集器外别无选择。由于单线程的老年代Serial Old收集器在服务端应用性能上的”拖累“，即使使用了Parallel Scavenge收集器也未必能在整体应用上获得吞吐量最大化的效果，又因为老年代收集中无法充分利用服务器多CPU的处理能力，在老年代很大而且硬件比较高级的环境中，这种组合的吞吐量甚至还不一定有ParNew加CMS的组合”给力“。直到Parallel Old收集器出现后，”吞吐量优先“收集器终于有了比较名副其实的应用祝贺，在注重吞吐量及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器。</p><p>-UseParallelGC: 虚拟机运行在Server模式下的默认值，打开此开关后，使用Parallel Scavenge + Serial Old的收集器组合进行内存回收。-UseParallelOldGC: 打开此开关后，使用Parallel Scavenge + Parallel Old的收集器组合进行垃圾回收</p><h3 id="CMS收集器"><a href="#CMS收集器" class="headerlink" title="CMS收集器"></a>CMS收集器</h3><p>CMS(Concurrent Mark Swep)收集器是一个比较重要的回收器，现在应用非常广泛，我们重点来看一下，CMS一种获取最短回收停顿时间为目标的收集器，这使得它很适合用于和用户交互的业务。从名字(Mark Swep)就可以看出，CMS收集器是基于标记清除算法实现的。它的收集过程分为四个步骤：</p><p>初始标记(initial mark)<br>并发标记(concurrent mark)<br>重新标记(remark)<br>并发清除(concurrent sweep)<br>注意初始标记和重新标记还是会stop the world，但是在耗费时间更长的并发标记和并发清除两个阶段都可以和用户进程同时工作。</p><p>不过由于CMS收集器是基于标记清除算法实现的，会导致有大量的空间碎片产生，在为大对象分配内存的时候，往往会出现老年代还有很大的空间剩余，但是无法找到足够大的连续空间来分配当前对象，不得不提前开启一次Full GC。为了解决这个问题，CMS收集器默认提供了一个-XX:+UseCMSCompactAtFullCollection收集开关参数（默认就是开启的)，用于在CMS收集器进行FullGC完开启内存碎片的合并整理过程，内存整理的过程是无法并发的，这样内存碎片问题倒是没有了，不过停顿时间不得不变长。虚拟机设计者还提供了另外一个参数-XX:CMSFullGCsBeforeCompaction参数用于设置执行多少次不压缩的Full GC后跟着来一次带压缩的（默认值为0，表示每次进入Full GC时都进行碎片整理）。</p><p>不幸的是，它作为老年代的收集器，却无法与jdk1.4中已经存在的新生代收集器Parallel Scavenge配合工作，所以在jdk1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或Serial收集器中的一个。ParNew收集器是使用-XX:+UseConcMarkSweepGC选项启用CMS收集器之后的默认新生代收集器，也可以使用-XX:+UseParNewGC选项来强制指定它。</p><p>由于CMS收集器现在比较常用，下面我们再额外了解一下CMS算法的几个常用参数：</p><p>UseCMSInitatingOccupancyOnly：表示只在到达阈值的时候，才进行 CMS 回收。<br>为了减少第二次暂停的时间，通过-XX:+CMSParallelRemarkEnabled开启并行remark。如果ramark时间还是过长的话，可以开启-XX:+CMSScavengeBeforeRemark选项，强制remark之前开启一次minor gc，减少remark的暂停时间，但是在remark之后也立即开始一次minor gc。<br>CMS默认启动的回收线程数目是(ParallelGCThreads + 3)/4，如果你需要明确设定，可以通过-XX:+ParallelCMSThreads来设定，其中-XX:+ParallelGCThreads代表的年轻代的并发收集线程数目。<br>CMSClassUnloadingEnabled： 允许对类元数据进行回收。<br>CMSInitatingPermOccupancyFraction：当永久区占用率达到这一百分比后，启动 CMS 回收 (前提是-XX:+CMSClassUnloadingEnabled 激活了)。<br>CMSIncrementalMode：使用增量模式，比较适合单CPU。<br>UseCMSCompactAtFullCollection参数可以使 CMS 在垃圾收集完成后，进行一次内存碎片整理。内存碎片的整理并不是并发进行的。<br>UseFullGCsBeforeCompaction：设定进行多少次 CMS 垃圾回收后，进行一次内存压缩。<br>一些建议<br>对于Native Memory:</p><p>使用了NIO或者NIO框架（Mina / Netty）<br>使用了DirectByteBuffer分配字节缓冲区<br>使用了MappedByteBuffer做内存映射<br>由于Native Memory只能通过FullGC回收，所以除非你非常清楚这时真的有必要，否则不要轻易调用System.gc()。</p><p>另外为了防止某些框架中的System.gc调用（例如NIO框架、Java RMI），建议在启动参数中加上-XX:+DisableExplicitGC来禁用显式GC。这个参数有个巨大的坑，如果你禁用了System.gc()，那么上面的3种场景下的内存就无法回收，可能造成OOM，如果你使用了CMS GC，那么可以用这个参数替代：-XX:+ExplicitGCInvokesConcurrent。</p><p>此外除了CMS的GC，其实其他针对old gen的回收器都会在对old gen回收的同时回收young gen。</p><h3 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h3><p>G1收集器是一款面向服务端应用的垃圾收集器。HotSpot团队赋予它的使命是在未来替换掉JDK1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点：</p><p>并行与并发：G1能更充分的利用CPU，多核环境下的硬件优势来缩短stop the world的停顿时间。<br>分代收集：和其他收集器一样，分代的概念在G1中依然存在，不过G1不需要其他的垃圾回收器的配合就可以独自管理整个GC堆。<br>空间整合：G1收集器有利于程序长时间运行，分配大对象时不会无法得到连续的空间而提前触发一次GC。<br>可预测的非停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒。<br>在使用G1收集器时，Java堆的内存布局和其他收集器有很大的差别，它将这个Java堆分为多个大小相等的独立区域，虽然还保留新生代和老年代的概念，但是新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。</p><p>虽然G1看起来有很多优点，实际上CMS还是主流。</p><h2 id="与GC相关的常用参数"><a href="#与GC相关的常用参数" class="headerlink" title="与GC相关的常用参数"></a>与GC相关的常用参数</h2><p>除了上面提及的一些参数，下面补充一些和GC相关的常用参数：</p><p>-Xmx: 设置堆内存的最大值。<br>-Xms: 设置堆内存的初始值。<br>-Xmn: 设置新生代的大小。<br>-Xss: 设置栈的大小。<br>-PretenureSizeThreshold: 直接晋升到老年代的对象大小，设置这个参数后，大于这个参数的对象将直接在老年代分配。<br>-MaxTenuringThrehold: 晋升到老年代的对象年龄。每个对象在坚持过一次Minor GC之后，年龄就会加1，当超过这个参数值时就进入老年代。<br>-UseAdaptiveSizePolicy: 在这种模式下，新生代的大小、eden 和 survivor 的比例、晋升老年代的对象年龄等参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。在手工调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量 (GCTimeRatio) 和停顿时间 (MaxGCPauseMills)，让虚拟机自己完成调优工作。<br>-SurvivorRattio: 新生代Eden区域与Survivor区域的容量比值，默认为8，代表Eden: Suvivor= 8: 1。<br>-XX:ParallelGCThreads：设置用于垃圾回收的线程数。通常情况下可以和 CPU 数量相等。但在 CPU 数量比较多的情况下，设置相对较小的数值也是合理的。<br>-XX:MaxGCPauseMills：设置最大垃圾收集停顿时间。它的值是一个大于 0 的整数。收集器在工作时，会调整 Java 堆大小或者其他一些参数，尽可能地把停顿时间控制在 MaxGCPauseMills 以内。<br>-XX:GCTimeRatio:设置吞吐量大小，它的值是一个 0-100 之间的整数。假设 GCTimeRatio 的值为 n，那么系统将花费不超过 1/(1+n) 的时间用于垃圾收集。</p>]]></content>
      
      <categories>
          
          <category> Java 虚拟机 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>about</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[<p><img src="/images/about/monkey.png" alt="Alt about"></p><h3 id="About-Me"><a href="#About-Me" class="headerlink" title="About Me"></a>About Me</h3><p><strong>“ The man who has begun to live more seriously within begins to live more simply without.”</strong></p><p>真诚、乐观的理想主义者，喜欢同样真诚、乐观的人。</p><blockquote><p>你可以通过下面的方式联系我。<br>E-mail: <a href="mailto:antipe@outlook.com" target="_blank" rel="noopener">antipe@outlook.com</a></p></blockquote>]]></content>
    </entry>
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
